{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpad.pyg.nets.pointnet2 as pnp_orig\n",
    "from open_anything_diffusion.models.modules.dit_models import DGDiT, DiT, PN2DiT, PN2HisDiT\n",
    "from open_anything_diffusion.models.modules.history_encoder import HistoryEncoder\n",
    "from open_anything_diffusion.models.flow_trajectory_predictor import FlowTrajectoryInferenceModule\n",
    "from open_anything_diffusion.models.flow_diffuser_dit import FlowTrajectoryDiffuserInferenceModule_DiT\n",
    "from open_anything_diffusion.models.flow_diffuser_dgdit import FlowTrajectoryDiffuserInferenceModule_DGDiT\n",
    "from open_anything_diffusion.models.flow_diffuser_hispndit import FlowTrajectoryDiffuserInferenceModule_HisPNDiT\n",
    "# from open_anything_diffusion.models.flow_diffuser_pndit import FlowTrajectoryDiffuserInferenceModule_PNDiT\n",
    "inference_module_class = {\n",
    "    \"dit\": FlowTrajectoryDiffuserInferenceModule_DiT,\n",
    "    \"dgdit\": FlowTrajectoryDiffuserInferenceModule_DGDiT,\n",
    "    \"hispndit\": FlowTrajectoryDiffuserInferenceModule_HisPNDiT,\n",
    "    \"flowbot\": FlowTrajectoryInferenceModule\n",
    "}\n",
    "networks = {\n",
    "    \"flowbot\": pnp_orig.PN2Dense(\n",
    "        in_channels=0,\n",
    "        out_channels=3,\n",
    "        p=pnp_orig.PN2DenseParams(),\n",
    "    ),\n",
    "    \"dit\": DiT(in_channels=6, depth=5, hidden_size=128, num_heads=4, learn_sigma=True).cuda(),\n",
    "    \"dgdit\": DGDiT(in_channels=3, depth=5, hidden_size=128, patch_size=1, num_heads=4, n_points=1200).cuda(),\n",
    "    \"hispndit\": {\n",
    "        \"DiT\": PN2HisDiT(\n",
    "            history_embed_dim=128,\n",
    "            in_channels=3,\n",
    "            depth=5,\n",
    "            hidden_size=128,\n",
    "            num_heads=4,\n",
    "            learn_sigma=True,\n",
    "        ).cuda(),\n",
    "        \"History\": HistoryEncoder(\n",
    "            history_dim=128,\n",
    "            history_len=1,\n",
    "            batch_norm=True,\n",
    "            transformer=False,\n",
    "            repeat_dim=False,\n",
    "        ).cuda(),\n",
    "    }\n",
    "    # \"pndit\": PN2DiT(in_channels=3, depth=5, hidden_size=128, patch_size=1, num_heads=4, n_points=1200),  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 1\n",
    "        self.trajectory_len = 1\n",
    "        self.mask_input_channel = False\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "class ModelConfig:\n",
    "    def __init__(self):\n",
    "        self.num_train_timesteps = 100\n",
    "\n",
    "model_config = ModelConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ckpt_dir = './pretrained'\n",
    "train_type = 'fullset_half_half'   # door_half_half, fullset_half_half - what dataset the model is trained on \n",
    "model_type = 'hispndit'   # dit, dgdit, hispndit - model structure\n",
    "ckpt_path = os.path.join(ckpt_dir, f'{train_type}_{model_type}.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = inference_module_class[model_type](\n",
    "    networks[model_type], inference_cfg=inference_config, model_cfg=model_config\n",
    ")\n",
    "model.load_from_ckpt(ckpt_path)\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pcd_dir = '/home/yishu/Azure_Kinect_ROS_Driver/src/pc_data_for_yishu'\n",
    "pcd_paths = [os.path.join(pcd_dir, pcd_name) for pcd_name in os.listdir(pcd_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 2\n",
    "# path = pcd_paths[id]\n",
    "# path = '/home/yishu/Azure_Kinect_ROS_Driver/src/pc_data_for_yishu/incorrect_toilet.npy'\n",
    "# path = '/home/yishu/Azure_Kinect_ROS_Driver/src/pc_data_for_yishu/fridge_L_open_45.npy'\n",
    "path = '/home/yishu/flowbot_panda/failure_case_with_additional_crop_world.npy'\n",
    "print(path)\n",
    "pcd = np.load(path) * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotate the point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rot = np.array([[0, -1, 0], [1,  0, 0], [0,  0, 1]])\n",
    "mean_x = pcd[:, 0].mean()\n",
    "mean_y = pcd[:, 1].mean()\n",
    "rot_pcd = pcd.copy()\n",
    "rot_pcd[:, 0] -= mean_x\n",
    "rot_pcd[:, 1] -= mean_y\n",
    "# rot_pcd[:, 2] += 1\n",
    "rot_pcd = rot_pcd@rot.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample it to 1200 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could use pytorch3d for this but it has some cuda conflict with my current env and I don't wnana change lol\n",
    "import numpy as np\n",
    "\n",
    "def farthest_point_sampling(world_points, points, k):\n",
    "    num_points = points.shape[0]\n",
    "    chosen_indices = np.zeros(k, dtype=int)\n",
    "    chosen_indices[0] = np.random.randint(num_points)\n",
    "    distances = np.full(num_points, np.inf)\n",
    "    \n",
    "    for i in range(1, k):\n",
    "        dist = np.linalg.norm(points - points[chosen_indices[i-1]], axis=1)\n",
    "        distances = np.minimum(distances, dist)\n",
    "        chosen_indices[i] = np.argmax(distances)\n",
    "        \n",
    "    return points[chosen_indices], world_points[chosen_indices]\n",
    "\n",
    "# Example usage\n",
    "# sampled_points = farthest_point_sampling(pcd, 1200)\n",
    "sampled_points, sampled_points_world = farthest_point_sampling(pcd, rot_pcd, 1200)\n",
    "\n",
    "print(sampled_points.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If don't use history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sampling\n",
    "# import torch\n",
    "# sampled_points = rot_pcd[np.random.randint(0, pcd.shape[0], 1200)]\n",
    "pred_flow = model.predict(sampled_points)[:, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If use history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_pcd = np.zeros_like(sampled_points)\n",
    "history_flow = np.zeros_like(sampled_points)\n",
    "pred_flow = model.predict(sampled_points, history_pcd=history_flow, history_flow=history_pcd)[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm\n",
    "# from open_anything_diffusion.metrics.trajectory import flow_metrics\n",
    "# gt_flow = torch.zeros(1200, 3)\n",
    "# gt_flow[:, 2] = 1 \n",
    "# cosines = []\n",
    "# for i in tqdm.tqdm(range(20)):\n",
    "#     sampled_points = rot_pcd[np.random.randint(0, pcd.shape[0], 1200)]\n",
    "#     pred_flow = model.predict(sampled_points)[:, 0, :]\n",
    "#     rmse, cos_dist, mag_error = flow_metrics(\n",
    "#         pred_flow, gt_flow, reduce=True\n",
    "#     )\n",
    "#     cosines.append(cos_dist)\n",
    "\n",
    "# print(max(cosines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_flow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# sampled_points = rot_pcd[np.random.randint(0, pcd.shape[0], 1200)]\n",
    "gt_action = torch.tensor([-1, 0, 0])\n",
    "cosines = []\n",
    "predictions = []\n",
    "for i in range(50):\n",
    "    pred_flow = model.predict(sampled_points)[:, 0, :]\n",
    "    idx = torch.topk(pred_flow, k=1)[1][0]\n",
    "    action = pred_flow[idx]\n",
    "    cosine = torch.cosine_similarity(action, gt_action)\n",
    "    cosines.append(cosine)\n",
    "    predictions.append(pred_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
    "animation = FlowNetAnimation()\n",
    "animation.add_trace(\n",
    "    torch.as_tensor(sampled_points),\n",
    "    # torch.as_tensor([pcd[mask]]),\n",
    "    # torch.as_tensor([flow[mask]]),\n",
    "    torch.as_tensor([sampled_points]),\n",
    "    # torch.as_tensor([pred_flow.cpu().numpy() * 2]),\n",
    "    torch.as_tensor([predictions[6].cpu().numpy()]* 5),\n",
    "    \"red\",\n",
    ")\n",
    "fig = animation.animate()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal point and orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation \n",
    "def get_contact_point_and_flow_vector(flow, xyz):\n",
    "    magnitude = torch.norm(flow, dim=1)\n",
    "    idx_of_max_flow = torch.argmax(magnitude.unsqueeze(1))\n",
    "    contact_point = torch.from_numpy(xyz[idx_of_max_flow])\n",
    "    flow_vector = flow[idx_of_max_flow] \n",
    "    flow_vector_normalized = (flow_vector / flow_vector.norm(dim=-1)).float()\n",
    "    return contact_point, flow_vector_normalized\n",
    "\n",
    "\n",
    "def get_goal_point_and_orientation(contact_point, flow_vector):\n",
    "    goal_point = contact_point + 0.2 * flow_vector\n",
    "    e_z_init = torch.tensor([0, 0, 1.0]).float().cuda()\n",
    "    e_y = -flow_vector\n",
    "    e_x = torch.linalg.cross(e_y, e_z_init)\n",
    "    e_x = e_x / e_x.norm(dim=-1)\n",
    "    e_z = torch.linalg.cross(e_x, e_y)\n",
    "    R_goal = torch.stack([e_x, e_y, e_z], dim=1).cuda()\n",
    "    R_gripper = torch.as_tensor(\n",
    "        [\n",
    "            [1, 0, 0],\n",
    "            [0, 0, 1.0],\n",
    "            [0, -1.0, 0],\n",
    "        ]\n",
    "    ).cuda()\n",
    "\n",
    "    goal_orientation_mat = (R_goal @ R_gripper).cpu()\n",
    "    goal_orientation_quat = Rotation.from_matrix((R_goal @ R_gripper).cpu()).as_quat()\n",
    "    return goal_point, goal_orientation_quat, goal_orientation_mat\n",
    "\n",
    "\n",
    "def transform_flow_contact_point_goal_point_and_orientation_to_world(contact_point, goal_point, goal_orientation, mean_x, mean_y, flow_vector):\n",
    "    # Formatting goal_point and goal_orientation to be in the same frame as the point cloud so that it can be visualized\n",
    "    R = torch.tensor([[0, 1, 0], [-1, 0, 0], [0, 0, 1]]).float().cuda()\n",
    "\n",
    "    goal_point = goal_point @ R.T\n",
    "    contact_point = contact_point @ R.T\n",
    "\n",
    "    # Add the mean in x and y\n",
    "    goal_point[0] += mean_x\n",
    "    goal_point[1] += mean_y\n",
    "    contact_point[0] += mean_x\n",
    "    contact_point[1] += mean_y\n",
    "    \n",
    "    goal_point = goal_point.cpu().numpy()\n",
    "    goal_point = np.reshape(goal_point, (1, 3))\n",
    "    contact_point = contact_point.cpu().numpy()\n",
    "    contact_point = np.reshape(contact_point, (1, 3))\n",
    "\n",
    "    goal_orientation = Rotation.from_quat(goal_orientation)\n",
    "    goal_orientation = torch.from_numpy(goal_orientation.as_matrix()).float().cuda()\n",
    "    goal_orientation = R @ goal_orientation\n",
    "    goal_orientation = Rotation.from_matrix(goal_orientation.cpu()).as_quat()\n",
    "\n",
    "    flow_vector = flow_vector @ R.T\n",
    "\n",
    "    return contact_point, goal_point, goal_orientation, flow_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "contact_point, flow_vector_normalized = get_contact_point_and_flow_vector(pred_flow, sampled_points)\n",
    "contact_point = contact_point.float().cuda()\n",
    "flow_vector_normalized = flow_vector_normalized.float().cuda()\n",
    "goal_point, goal_orientation_quat, goal_orientation_mat = get_goal_point_and_orientation(contact_point, flow_vector_normalized)\n",
    "# contact_point_world, goal_point_world, goal_orientation_world,flow_vector = transform_flow_contact_point_goal_point_and_orientation_to_world(contact_point, goal_point, goal_orientation_quat, mean_x, mean_y, flow_vector_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_vector_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(contact_point)\n",
    "print(goal_point)\n",
    "print(goal_orientation_quat)\n",
    "print(goal_orientation_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# import open3d as o3d\n",
    "# goal_orientation_frame = (o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1, origin=goal_point[0]))\n",
    "# goal_orientation_frame_r = copy.deepcopy(goal_orientation_frame)\n",
    "# rot = goal_orientation_frame.get_rotation_matrix_from_quaternion(goal_orientation_quat)\n",
    "# goal_orientation_frame_r.rotate(rot, center=goal_point[0])\n",
    "# mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.6, origin=[0, 0, 0])\n",
    "\n",
    "# pc = o3d.geometry.PointCloud()\n",
    "# pc.points = o3d.utility.Vector3dVector(sampled_points)  # Random points\n",
    "\n",
    "# # Create a visualizer object and add the point cloud\n",
    "# vis = o3d.visualization.Visualizer()\n",
    "# vis.create_window()\n",
    "# vis.add_geometry(goal_orientation_frame_r)\n",
    "# vis.add_geometry(pc)\n",
    "# vis.run()\n",
    "# vis.destroy_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flowbot frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
    "\n",
    "magnitude = torch.norm(pred_flow, dim=1)\n",
    "idx_of_max_flow = torch.argmax(magnitude)\n",
    "# print(idx_of_max_flow, pred_flow[idx_of_max_flow])\n",
    "masked_pred_flow = pred_flow.clone() * 0.1\n",
    "masked_pred_flow[idx_of_max_flow, :] *= 20\n",
    "print(idx_of_max_flow, masked_pred_flow[idx_of_max_flow], goal_orientation_mat)\n",
    "\n",
    "animation = FlowNetAnimation()\n",
    "animation.add_trace(\n",
    "    torch.as_tensor(sampled_points),\n",
    "    # torch.as_tensor([pcd[mask]]),\n",
    "    # torch.as_tensor([flow[mask]]),\n",
    "    torch.as_tensor([sampled_points]),\n",
    "    torch.as_tensor([masked_pred_flow.numpy()]),\n",
    "    \"red\",\n",
    ")\n",
    "animation.add_trace(\n",
    "    torch.as_tensor(goal_point.unsqueeze(0).repeat(3, 1).cpu().numpy()),\n",
    "    # torch.as_tensor([pcd[mask]]),\n",
    "    # torch.as_tensor([flow[mask]]),\n",
    "    torch.as_tensor([goal_point.unsqueeze(0).repeat(3, 1).cpu().numpy()]),\n",
    "    torch.as_tensor([goal_orientation_mat.T.cpu().numpy()]),\n",
    "    \"yellow\",\n",
    ")\n",
    "\n",
    "fig = animation.animate()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(contact_point)\n",
    "print(goal_point)\n",
    "print(goal_orientation_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### World frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_point_world, goal_point_world, goal_orientation_world, flow_vector = transform_flow_contact_point_goal_point_and_orientation_to_world(contact_point, goal_point, goal_orientation_quat, mean_x, mean_y, flow_vector_normalized)\n",
    "goal_orientation_mat_world = Rotation.from_quat(goal_orientation_world).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(contact_point_world)\n",
    "print(goal_point_world)\n",
    "print(goal_orientation_mat_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
    "\n",
    "# magnitude = torch.norm(pred_flow, dim=1)\n",
    "# idx_of_max_flow = torch.argmax(magnitude)\n",
    "masked_pred_flow = pred_flow.clone() * 0.1\n",
    "masked_pred_flow[idx_of_max_flow, :] *= 20\n",
    "\n",
    "R = torch.tensor([[0, 1, 0], [-1, 0, 0], [0, 0, 1]]).float()\n",
    "\n",
    "masked_pred_flow_world = masked_pred_flow @ R.T\n",
    "print(idx_of_max_flow, masked_pred_flow_world[idx_of_max_flow], goal_orientation_mat_world)\n",
    "\n",
    "animation = FlowNetAnimation()\n",
    "animation.add_trace(\n",
    "    torch.as_tensor(sampled_points_world),\n",
    "    # torch.as_tensor([pcd[mask]]),\n",
    "    # torch.as_tensor([flow[mask]]),\n",
    "    torch.as_tensor([sampled_points_world]),\n",
    "    torch.as_tensor([masked_pred_flow_world.numpy()]),\n",
    "    \"red\",\n",
    ")\n",
    "animation.add_trace(\n",
    "    torch.as_tensor(torch.from_numpy(goal_point_world).repeat(3, 1).cpu().numpy()),\n",
    "    # torch.as_tensor([pcd[mask]]),\n",
    "    # torch.as_tensor([flow[mask]]),\n",
    "    torch.as_tensor([torch.from_numpy(goal_point_world).repeat(3, 1).cpu().numpy()]),\n",
    "    torch.as_tensor([goal_orientation_mat_world.T]),\n",
    "    \"yellow\",\n",
    ")\n",
    "\n",
    "fig = animation.animate()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Define vectors\n",
    "vectors = [\n",
    "    dict(x=[0, 3], y=[0, 2], z=[0, 1], name='Vector 1'),\n",
    "    dict(x=[0, 1], y=[0, 2], z=[0, 3], name='Vector 2'),\n",
    "    dict(x=[0, 1], y=[0, 0], z=[0, 1], name='Vector 3')\n",
    "]\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add vectors to plot\n",
    "for v in vectors:\n",
    "    fig.add_trace(go.Scatter3d(x=v['x'], y=v['y'], z=v['z'], mode='lines+markers+text', name=v['name']))\n",
    "\n",
    "# Update layout for a nice aspect ratio\n",
    "fig.update_layout(scene=dict(\n",
    "    xaxis=dict(nticks=4, range=[-5,5]),\n",
    "    yaxis=dict(nticks=4, range=[-5,5]),\n",
    "    zaxis=dict(nticks=4, range=[-5,5])\n",
    "), width=700)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(contact_point)\n",
    "print(goal_point, goal_orientation)\n",
    "print(contact_point, goal_point, goal_orientation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flow_vector_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
    "animation = FlowNetAnimation()\n",
    "animation.add_trace(\n",
    "    torch.as_tensor(pcd),\n",
    "    # torch.as_tensor([pcd[mask]]),\n",
    "    # torch.as_tensor([flow[mask]]),\n",
    "    torch.as_tensor([pcd]),\n",
    "    torch.as_tensor([np.zeros_like(pcd)]),\n",
    "    \"red\",\n",
    ")\n",
    "fig = animation.animate()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Switch grasp point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo codes\n",
    "def switch_grasp_point(last_gripper_pos, current_gripper_pos, flow_prediction, current_pcd):\n",
    "    # 1 - find the point in current_pcd closest to current_grasp_point\n",
    "    grasp_point_id = 0  # current_pcd's closest point id\n",
    "    grasp_flow = flow_prediction[grasp_point_id]\n",
    "    # 2 - Compare the grasp point flow with the max prediction flow\n",
    "    leverage_increase = flow_prediction.norm(dim=-1).max() - grasp_flow.norm()\n",
    "    if last_gripper_pos - current_gripper_pos < 0.01 or leverage_increase > 0.2:  # move threshold\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use / not use history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically we want to have a signal that tells us whether the last step makes positive progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy: Maintain a correct direction stack\n",
    "\n",
    "For each step, record the movement of the gripper (delta gripper)\n",
    "\n",
    "1) When the stack is empty, only push the delta gripper in the stack when the |delta gripper| > threshold (means that the gripper successfully moves the object)\n",
    "\n",
    "2) Always compare the current predicted direction with the direction from the stack top: \n",
    "\n",
    "- if different by over 90 degree, don't push the current action in and also don't execute this step\n",
    "- Execute the step and push the delta gripper of this step in the stack"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openany",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
