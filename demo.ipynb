{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_anything_diffusion.models.modules.dit_models import DGDiT, DiT, PN2DiT\n",
    "from open_anything_diffusion.models.flow_diffuser_dit import FlowTrajectoryDiffuserInferenceModule_DiT\n",
    "from open_anything_diffusion.models.flow_diffuser_dgdit import FlowTrajectoryDiffuserInferenceModule_DGDiT\n",
    "# from open_anything_diffusion.models.flow_diffuser_pndit import FlowTrajectoryDiffuserInferenceModule_PNDiT\n",
    "inference_module_class = {\n",
    "    \"dit\": FlowTrajectoryDiffuserInferenceModule_DiT,\n",
    "    \"dgdit\": FlowTrajectoryDiffuserInferenceModule_DGDiT,\n",
    "    # \"pndit\": FlowTrajectoryDiffuserInferenceModule_PNDiT\n",
    "}\n",
    "networks = {\n",
    "    \"dit\": DiT(in_channels=6, depth=5, hidden_size=128, num_heads=4, learn_sigma=True),\n",
    "    \"dgdit\": DGDiT(in_channels=3, depth=5, hidden_size=128, patch_size=1, num_heads=4, n_points=1200),\n",
    "    # \"pndit\": PN2DiT(in_channels=3, depth=5, hidden_size=128, patch_size=1, num_heads=4, n_points=1200),  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 1\n",
    "        self.trajectory_len = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "class ModelConfig:\n",
    "    def __init__(self):\n",
    "        self.num_train_timesteps = 100\n",
    "\n",
    "model_config = ModelConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ckpt_dir = './pretrained'\n",
    "train_type = 'fullset_half_half'   # door_half_half, fullset_half_half - what dataset the model is trained on \n",
    "model_type = 'dit'   # dit, dgdit - model structure\n",
    "ckpt_path = os.path.join(ckpt_dir, f'{train_type}_{model_type}.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = inference_module_class[model_type](\n",
    "    networks[model_type].cuda(), inference_cfg=inference_config, model_cfg=model_config\n",
    ")\n",
    "model.load_from_ckpt(ckpt_path)\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pcd_dir = '/home/yishu/Azure_Kinect_ROS_Driver/src/pc_data_for_yishu'\n",
    "pcd_paths = [os.path.join(pcd_dir, pcd_name) for pcd_name in os.listdir(pcd_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 2\n",
    "# path = pcd_paths[id]\n",
    "path = '/home/yishu/Azure_Kinect_ROS_Driver/src/pc_data_for_yishu/incorrect_toilet.npy'\n",
    "# path = '/home/yishu/Azure_Kinect_ROS_Driver/src/pc_data_for_yishu/fridge_L_open_15.npy'\n",
    "print(path)\n",
    "pcd = np.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotate the point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rot = np.array([[0, -1, 0], [1,  0, 0], [0,  0, 1]])\n",
    "mean_x = pcd[:, 0].mean()\n",
    "mean_y = pcd[:, 1].mean()\n",
    "rot_pcd = pcd.copy()\n",
    "rot_pcd[:, 0] -= mean_x\n",
    "rot_pcd[:, 1] -= mean_y\n",
    "# rot_pcd[:, 2] += 1\n",
    "rot_pcd = rot_pcd@rot.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample it to 1200 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could use pytorch3d for this but it has some cuda conflict with my current env and I don't wnana change lol\n",
    "import numpy as np\n",
    "\n",
    "def farthest_point_sampling(points, k):\n",
    "    num_points = points.shape[0]\n",
    "    chosen_indices = np.zeros(k, dtype=int)\n",
    "    chosen_indices[0] = np.random.randint(num_points)\n",
    "    distances = np.full(num_points, np.inf)\n",
    "    \n",
    "    for i in range(1, k):\n",
    "        dist = np.linalg.norm(points - points[chosen_indices[i-1]], axis=1)\n",
    "        distances = np.minimum(distances, dist)\n",
    "        chosen_indices[i] = np.argmax(distances)\n",
    "        \n",
    "    return points[chosen_indices]\n",
    "\n",
    "# Example usage\n",
    "# sampled_points = farthest_point_sampling(pcd, 1200)\n",
    "sampled_points = farthest_point_sampling(rot_pcd, 1200)\n",
    "\n",
    "print(sampled_points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "sampled_points = rot_pcd[np.random.randint(0, pcd.shape[0], 1200)]\n",
    "pred_flow = model.predict(sampled_points)[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm\n",
    "# from open_anything_diffusion.metrics.trajectory import flow_metrics\n",
    "# gt_flow = torch.zeros(1200, 3)\n",
    "# gt_flow[:, 2] = 1 \n",
    "# cosines = []\n",
    "# for i in tqdm.tqdm(range(20)):\n",
    "#     sampled_points = rot_pcd[np.random.randint(0, pcd.shape[0], 1200)]\n",
    "#     pred_flow = model.predict(sampled_points)[:, 0, :]\n",
    "#     rmse, cos_dist, mag_error = flow_metrics(\n",
    "#         pred_flow, gt_flow, reduce=True\n",
    "#     )\n",
    "#     cosines.append(cos_dist)\n",
    "\n",
    "# print(max(cosines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
    "animation = FlowNetAnimation()\n",
    "animation.add_trace(\n",
    "    torch.as_tensor(sampled_points),\n",
    "    # torch.as_tensor([pcd[mask]]),\n",
    "    # torch.as_tensor([flow[mask]]),\n",
    "    torch.as_tensor([sampled_points]),\n",
    "    torch.as_tensor([pred_flow.cpu().numpy()]),\n",
    "    \"red\",\n",
    ")\n",
    "fig = animation.animate()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal point and orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_of_max_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_pred_flow[idx_of_max_flow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
    "\n",
    "magnitude = torch.norm(pred_flow, dim=1)\n",
    "idx_of_max_flow = torch.argmax(magnitude)\n",
    "masked_pred_flow = pred_flow.clone() * 0.01\n",
    "masked_pred_flow[idx_of_max_flow, :] = 2\n",
    "\n",
    "animation = FlowNetAnimation()\n",
    "animation.add_trace(\n",
    "    torch.as_tensor(sampled_points),\n",
    "    # torch.as_tensor([pcd[mask]]),\n",
    "    # torch.as_tensor([flow[mask]]),\n",
    "    torch.as_tensor([sampled_points]),\n",
    "    torch.as_tensor([masked_pred_flow.numpy()]),\n",
    "    \"red\",\n",
    ")\n",
    "fig = animation.animate()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation \n",
    "def get_contact_point_and_flow_vector(flow, xyz):\n",
    "    magnitude = torch.norm(flow, dim=1)\n",
    "    idx_of_max_flow = torch.argmax(magnitude.unsqueeze(1))\n",
    "    contact_point = torch.from_numpy(xyz[idx_of_max_flow])\n",
    "    flow_vector = flow[idx_of_max_flow] \n",
    "    flow_vector_normalized = (flow_vector / flow_vector.norm(dim=-1)).float()\n",
    "    return contact_point, flow_vector_normalized\n",
    "\n",
    "\n",
    "def get_goal_point_and_orientation(contact_point, flow_vector):\n",
    "    goal_point = contact_point + 0.2 * flow_vector\n",
    "    e_z_init = torch.tensor([0, 0, 1.0]).float().cuda()\n",
    "    e_y = -flow_vector\n",
    "    e_x = torch.linalg.cross(e_y, e_z_init)\n",
    "    e_x = e_x / e_x.norm(dim=-1)\n",
    "    e_z = torch.linalg.cross(e_x, e_y)\n",
    "    R_goal = torch.stack([e_x, e_y, e_z], dim=1).cuda()\n",
    "    R_gripper = torch.as_tensor(\n",
    "        [\n",
    "            [1, 0, 0],\n",
    "            [0, 0, 1.0],\n",
    "            [0, -1.0, 0],\n",
    "        ]\n",
    "    ).cuda()\n",
    "\n",
    "    goal_orientation = Rotation.from_matrix((R_goal @ R_gripper).cpu()).as_quat()\n",
    "    return goal_point, goal_orientation\n",
    "\n",
    "\n",
    "def transform_flow_contact_point_goal_point_and_orientation_to_world(contact_point, goal_point, goal_orientation, mean_x, mean_y, flow_vector):\n",
    "    # Formatting goal_point and goal_orientation to be in the same frame as the point cloud so that it can be visualized\n",
    "    R = torch.tensor([[0, 1, 0], [-1, 0, 0], [0, 0, 1]]).float().cuda()\n",
    "\n",
    "    # Add the mean in x and y\n",
    "    goal_point[0] += mean_x\n",
    "    goal_point[1] += mean_y\n",
    "    contact_point[0] += mean_x\n",
    "    contact_point[1] += mean_y\n",
    "\n",
    "    goal_point = goal_point @ R.T\n",
    "    contact_point = contact_point @ R.T\n",
    "    \n",
    "    goal_point = goal_point.cpu().numpy()\n",
    "    goal_point = np.reshape(goal_point, (1, 3))\n",
    "    contact_point = contact_point.cpu().numpy()\n",
    "    contact_point = np.reshape(contact_point, (1, 3))\n",
    "\n",
    "    goal_orientation = Rotation.from_quat(goal_orientation)\n",
    "    goal_orientation = torch.from_numpy(goal_orientation.as_matrix()).float().cuda()\n",
    "    goal_orientation = goal_orientation @ R.T\n",
    "    goal_orientation = Rotation.from_matrix(goal_orientation.cpu()).as_quat()\n",
    "\n",
    "    flow_vector = flow_vector @ R.T\n",
    "\n",
    "    return contact_point, goal_point, goal_orientation, flow_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_point, flow_vector_normalized = get_contact_point_and_flow_vector(pred_flow, sampled_points)\n",
    "contact_point = contact_point.float().cuda()\n",
    "flow_vector_normalized = flow_vector_normalized.float().cuda()\n",
    "goal_point, goal_orientation = get_goal_point_and_orientation(contact_point, flow_vector_normalized)\n",
    "contact_point, goal_point, goal_orientation,flow_vector = transform_flow_contact_point_goal_point_and_orientation_to_world(contact_point, goal_point, goal_orientation, mean_x, mean_y, flow_vector_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Define vectors\n",
    "vectors = [\n",
    "    dict(x=[0, 3], y=[0, 2], z=[0, 1], name='Vector 1'),\n",
    "    dict(x=[0, 1], y=[0, 2], z=[0, 3], name='Vector 2'),\n",
    "    dict(x=[0, 1], y=[0, 0], z=[0, 1], name='Vector 3')\n",
    "]\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add vectors to plot\n",
    "for v in vectors:\n",
    "    fig.add_trace(go.Scatter3d(x=v['x'], y=v['y'], z=v['z'], mode='lines+markers+text', name=v['name']))\n",
    "\n",
    "# Update layout for a nice aspect ratio\n",
    "fig.update_layout(scene=dict(\n",
    "    xaxis=dict(nticks=4, range=[-5,5]),\n",
    "    yaxis=dict(nticks=4, range=[-5,5]),\n",
    "    zaxis=dict(nticks=4, range=[-5,5])\n",
    "), width=700)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(contact_point)\n",
    "print(goal_point, goal_orientation)\n",
    "print(contact_point, goal_point, goal_orientation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flow_vector_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
    "animation = FlowNetAnimation()\n",
    "animation.add_trace(\n",
    "    torch.as_tensor(pcd),\n",
    "    # torch.as_tensor([pcd[mask]]),\n",
    "    # torch.as_tensor([flow[mask]]),\n",
    "    torch.as_tensor([pcd]),\n",
    "    torch.as_tensor([np.zeros_like(pcd)]),\n",
    "    \"red\",\n",
    ")\n",
    "fig = animation.animate()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo codes\n",
    "def switch_grasp_point(last_gripper_pos, current_gripper_pos, flow_prediction, current_pcd):\n",
    "    # 1 - find the point in current_pcd closest to current_grasp_point\n",
    "    grasp_point_id = 0  # current_pcd's closest point id\n",
    "    grasp_flow = flow_prediction[grasp_point_id]\n",
    "    # 2 - Compare the grasp point flow with the max prediction flow\n",
    "    leverage_increase = flow_prediction.norm(dim=-1).max() - grasp_flow.norm()\n",
    "    if last_gripper_pos - current_gripper_pos < 0.01 or leverage_increase > 0.2:  # move threshold\n",
    "        return True\n",
    "    return False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openany",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
