{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_anything_diffusion.models.modules.dit_models import DGDiT, DiT\n",
    "from open_anything_diffusion.models.flow_diffuser_dit import FlowTrajectoryDiffuserInferenceModule_DiT\n",
    "from open_anything_diffusion.models.flow_diffuser_dgdit import FlowTrajectoryDiffuserInferenceModule_DGDiT\n",
    "inference_module_class = {\n",
    "    \"dit\": FlowTrajectoryDiffuserInferenceModule_DiT,\n",
    "    \"dgdit\": FlowTrajectoryDiffuserInferenceModule_DGDiT,\n",
    "}\n",
    "networks = {\n",
    "    \"dit\": DiT(in_channels=6, depth=5, hidden_size=128, num_heads=4, learn_sigma=True),\n",
    "    \"dgdit\": DGDiT(in_channels=3, depth=5, hidden_size=128, patch_size=1, num_heads=4, n_points=1200),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 1\n",
    "        self.trajectory_len = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "class ModelConfig:\n",
    "    def __init__(self):\n",
    "        self.num_train_timesteps = 100\n",
    "\n",
    "model_config = ModelConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ckpt_dir = './pretrained'\n",
    "train_type = 'fullset_half_half'   # door_half_half, fullset_half_half - what dataset the model is trained on \n",
    "model_type = 'dit'   # dit, dgdit - model structure\n",
    "ckpt_path = os.path.join(ckpt_dir, f'{train_type}_{model_type}.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = inference_module_class[model_type](\n",
    "    networks[model_type].cuda(), inference_cfg=inference_config, model_cfg=model_config\n",
    ").cuda()\n",
    "model.load_from_ckpt(ckpt_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pcd_dir = '/home/yishu/Azure_Kinect_ROS_Driver/src/pc_data_for_yishu'\n",
    "pcd_paths = [os.path.join(pcd_dir, pcd_name) for pcd_name in os.listdir(pcd_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 2\n",
    "# path = pcd_paths[id]\n",
    "path = '/home/yishu/Azure_Kinect_ROS_Driver/src/pc_data_for_yishu/fridge_L_open_fully.npy'\n",
    "print(path)\n",
    "pcd = np.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample it to 1200 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could use pytorch3d for this but it has some cuda conflict with my current env and I don't wnana change lol\n",
    "import numpy as np\n",
    "\n",
    "def farthest_point_sampling(points, k):\n",
    "    num_points = points.shape[0]\n",
    "    chosen_indices = np.zeros(k, dtype=int)\n",
    "    chosen_indices[0] = np.random.randint(num_points)\n",
    "    distances = np.full(num_points, np.inf)\n",
    "    \n",
    "    for i in range(1, k):\n",
    "        dist = np.linalg.norm(points - points[chosen_indices[i-1]], axis=1)\n",
    "        distances = np.minimum(distances, dist)\n",
    "        chosen_indices[i] = np.argmax(distances)\n",
    "        \n",
    "    return points[chosen_indices]\n",
    "\n",
    "# Example usage\n",
    "sampled_points = farthest_point_sampling(pcd, 1200)\n",
    "print(sampled_points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "pred_flow = model.predict(sampled_points)[:, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
    "animation = FlowNetAnimation()\n",
    "animation.add_trace(\n",
    "    torch.as_tensor(sampled_points),\n",
    "    # torch.as_tensor([pcd[mask]]),\n",
    "    # torch.as_tensor([flow[mask]]),\n",
    "    torch.as_tensor([sampled_points]),\n",
    "    torch.as_tensor([pred_flow.cpu().numpy()]),\n",
    "    \"red\",\n",
    ")\n",
    "fig = animation.animate()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
    "animation = FlowNetAnimation()\n",
    "animation.add_trace(\n",
    "    torch.as_tensor(pcd),\n",
    "    # torch.as_tensor([pcd[mask]]),\n",
    "    # torch.as_tensor([flow[mask]]),\n",
    "    torch.as_tensor([pcd]),\n",
    "    torch.as_tensor([np.zeros_like(pcd)]),\n",
    "    \"red\",\n",
    ")\n",
    "fig = animation.animate()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo codes\n",
    "def switch_grasp_point(last_gripper_pos, current_gripper_pos, flow_prediction, current_pcd):\n",
    "    # 1 - find the point in current_pcd closest to current_grasp_point\n",
    "    grasp_point_id = 0  # current_pcd's closest point id\n",
    "    grasp_flow = flow_prediction[grasp_point_id]\n",
    "    # 2 - Compare the grasp point flow with the max prediction flow\n",
    "    leverage_increase = flow_prediction.norm(dim=-1).max() - grasp_flow.norm()\n",
    "    if last_gripper_pos - current_gripper_pos < 0.01 or leverage_increase > 0.2:  # move threshold\n",
    "        return True\n",
    "    return False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openany",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
