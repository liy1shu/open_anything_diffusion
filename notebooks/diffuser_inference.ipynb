{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"/home/yishu/open_anything_diffusion/logs/train_trajectory/2023-10-18/08-45-11/checkpoints/epoch=74-step=15000-val_loss=0.00-weights-only.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_anything_diffusion.models.flow_trajectory_diffuser import (\n",
    "    FlowTrajectoryDiffusionModule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "initialize(config_path=\"../configs\", version_base=\"1.3\")\n",
    "cfg = compose(config_name=\"train_synthetic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpad.pyg.nets.pointnet2 as pnp\n",
    "network = pnp.PN2Dense(\n",
    "    in_channels=67,\n",
    "    out_channels=3,\n",
    "    p=pnp.PN2DenseParams(),\n",
    ")\n",
    "\n",
    "model = FlowTrajectoryDiffusionModule(network, cfg.training, cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt = torch.load(ckpt_path)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.loader as tgl\n",
    "from open_anything_diffusion.datasets.flow_trajectory_dataset_pyg import FlowTrajectoryPyGDataset\n",
    "from open_anything_diffusion.datasets.flow_trajectory import FlowTrajectoryDataModule\n",
    "datamodule = FlowTrajectoryDataModule(\n",
    "    root=\"/home/yishu/datasets/partnet-mobility\",\n",
    "    batch_size=1,\n",
    "    num_workers=30,\n",
    "    n_proc=2,\n",
    "    seed=42,\n",
    "    trajectory_len=cfg.training.trajectory_len,  # Only used when training trajectory model\n",
    "    toy_dataset = {\n",
    "        \"id\": \"door-1\",\n",
    "        \"train-train\": [\"8994\", \"9035\"],\n",
    "        \"train-test\": [\"8994\", \"9035\"],\n",
    "        \"test\": [\"8867\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "train_dataloader = datamodule.train_dataloader()\n",
    "val_dataloader = datamodule.train_val_dataloader()\n",
    "unseen_dataloader = datamodule.unseen_dataloader()\n",
    "\n",
    "# datamodule = FlowTrajectoryPyGDataset(\n",
    "#     root=\"/home/yishu/datasets/partnet-mobility/raw\",\n",
    "#     split=\"umpnet-train-test\",\n",
    "#     randomize_joints=True,\n",
    "#     randomize_camera=True,\n",
    "#     # batch_size=1,\n",
    "#     # num_workers=30,\n",
    "#     # n_proc=2,\n",
    "#     seed=42,\n",
    "#     trajectory_len=cfg.training.trajectory_len,  # Only used when training trajectory model\n",
    "# )\n",
    "# unseen_dataloader = tgl.DataLoader(datamodule, 1, shuffle=False, num_workers=0)\n",
    "\n",
    "samples = list(enumerate(val_dataloader))\n",
    "# # breakpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_anything_diffusion.metrics.trajectory import artflownet_loss, flow_metrics, normalize_trajectory\n",
    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
    "import numpy as np\n",
    "\n",
    "@torch.no_grad()\n",
    "def diffuse_visual(initial_noise, batch, model):  # 1 sample batch\n",
    "    model.eval()\n",
    "    \n",
    "    animation = FlowNetAnimation()\n",
    "    pcd = batch.pos.cpu().numpy()\n",
    "    mask = batch.mask.cpu().long().numpy()\n",
    "\n",
    "\n",
    "    bs = batch.delta.shape[0] // 1200\n",
    "    # batch.traj_noise = torch.randn_like(batch.delta, device=\"cuda\")\n",
    "    batch.traj_noise = initial_noise\n",
    "    # batch.traj_noise = normalize_trajectory(batch.traj_noise)\n",
    "    # breakpoint()\n",
    "\n",
    "    # import time\n",
    "    # batch_time = 0\n",
    "    # model_time = 0\n",
    "    # noise_scheduler_time = 0\n",
    "    # self.noise_scheduler_inference.set_timesteps(self.num_inference_timesteps)\n",
    "    # print(self.noise_scheduler_inference.timesteps)\n",
    "    # for t in self.noise_scheduler_inference.timesteps:\n",
    "    for t in model.noise_scheduler.timesteps:\n",
    "        \n",
    "        # tm = time.time()\n",
    "        batch.timesteps = torch.zeros(bs, device=model.device) + t  # Uniform t steps\n",
    "        batch.timesteps = batch.timesteps.long()\n",
    "        # batch_time += time.time() - tm\n",
    "\n",
    "        # tm = time.time()\n",
    "        model_output = model(batch)          # bs * 1200, traj_len * 3\n",
    "        model_output = model_output.reshape(model_output.shape[0], -1, 3)  # bs * 1200, traj_len, 3\n",
    "\n",
    "        print(model_output)\n",
    "        \n",
    "        batch.traj_noise = model.noise_scheduler.step(\n",
    "            # batch.traj_noise = self.noise_scheduler_inference.step(\n",
    "            model_output.reshape(\n",
    "                -1, model.sample_size, model_output.shape[1], model_output.shape[2]\n",
    "            ),\n",
    "            t,\n",
    "            batch.traj_noise.reshape(\n",
    "                -1, model.sample_size, model_output.shape[1], model_output.shape[2]\n",
    "            ),\n",
    "        ).prev_sample\n",
    "        batch.traj_noise = torch.flatten(batch.traj_noise, start_dim=0, end_dim=1)\n",
    "\n",
    "        # print(batch.traj_noise)\n",
    "        if t % 1 == 0:\n",
    "            flow = batch.traj_noise.squeeze().cpu().numpy()\n",
    "            # print(flow[mask])\n",
    "            # segmented_flow = np.zeros_like(flow, dtype=np.float32)\n",
    "            # segmented_flow[mask] = flow[mask]\n",
    "            # print(\"seg\", segmented_flow, \"flow\", flow)\n",
    "            animation.add_trace(\n",
    "                torch.as_tensor(pcd),\n",
    "                # torch.as_tensor([pcd[mask]]),\n",
    "                # torch.as_tensor([flow[mask]]),\n",
    "                torch.as_tensor([pcd]),\n",
    "                torch.as_tensor([flow]),\n",
    "                \"red\",\n",
    "            )\n",
    "            # animation.append_gif_frame(f)\n",
    "\n",
    "    f_pred = batch.traj_noise\n",
    "    f_pred = normalize_trajectory(f_pred)\n",
    "    # largest_mag: float = torch.linalg.norm(\n",
    "    #     f_pred, ord=2, dim=-1\n",
    "    # ).max()\n",
    "    # f_pred = f_pred / (largest_mag + 1e-6)\n",
    "\n",
    "    # Compute the loss.\n",
    "    n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(\"cuda\")  # type: ignore\n",
    "    f_ix = batch.mask.bool()\n",
    "    f_target = batch.delta\n",
    "    f_target = normalize_trajectory(f_target)\n",
    "\n",
    "    f_target = f_target.float()\n",
    "    # loss = artflownet_loss(f_pred, f_target, n_nodes)\n",
    "\n",
    "    # Compute some metrics on flow-only regions.\n",
    "    rmse, cos_dist, mag_error = flow_metrics(\n",
    "        f_pred[f_ix], batch.delta[f_ix]\n",
    "    )\n",
    "\n",
    "    return cos_dist, animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[0][1].pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = samples[1][1].cuda()\n",
    "batch = sample\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_noise = torch.randn_like(batch.delta, device=\"cuda\")\n",
    "cos_dist, animation = diffuse_visual(initial_noise, batch, model)\n",
    "fig = animation.animate()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "for i in tqdm.tqdm(range(100)):\n",
    "    initial_noise = torch.randn_like(batch.delta, device=\"cuda\")\n",
    "    cos_dist, animation = diffuse_visual(initial_noise, batch, model)\n",
    "    if cos_dist < -0.7:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = animation.animate()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "for i in tqdm.tqdm(range(100)):\n",
    "    initial_noise = torch.randn_like(batch.delta, device=\"cuda\")\n",
    "    cos_dist, animation = diffuse_visual(initial_noise, batch, model)\n",
    "    if cos_dist > 0.5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = animation.animate()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find multimodal cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "repeat_times = 10\n",
    "stop = False\n",
    "for sample in tqdm.tqdm(samples):\n",
    "    sample_id = sample[0]\n",
    "    sample = sample[1]\n",
    "    if stop:\n",
    "        break\n",
    "    batch = sample.cuda()\n",
    "    has_correct = False\n",
    "    has_incorrect = False\n",
    "    for _ in range(repeat_times):\n",
    "        cos_dist, animation = diffuse_visual(batch, model)\n",
    "        if cos_dist > 0.7:\n",
    "            has_correct = True\n",
    "            correct_animation = animation\n",
    "        elif cos_dist < 0: \n",
    "            has_incorrect = True\n",
    "            incorrect_animation = animation\n",
    "    if has_correct and has_incorrect:\n",
    "        print(sample_id, sample_id)\n",
    "        stop = True\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openany",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
