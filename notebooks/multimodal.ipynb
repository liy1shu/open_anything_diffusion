{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpad.partnet_mobility_utils.dataset as rpd\n",
    "all_objs = (\n",
    "    rpd.UMPNET_TEST_OBJS\n",
    ")\n",
    "id_to_obj_class = {obj_id: obj_class for obj_id, obj_class in all_objs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(id_to_obj_class.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "initialize(config_path=\"../configs\", version_base=\"1.3\")\n",
    "cfg = compose(config_name=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.loader as tgl\n",
    "from open_anything_diffusion.datasets.flow_trajectory_dataset_pyg import FlowTrajectoryPyGDataset\n",
    "datamodule = FlowTrajectoryPyGDataset(\n",
    "    root=\"/home/yishu/datasets/partnet-mobility/raw\",\n",
    "    split=\"umpnet-test\",\n",
    "    randomize_joints=True,\n",
    "    randomize_camera=True,\n",
    "    # batch_size=1,\n",
    "    # num_workers=30,\n",
    "    # n_proc=2,\n",
    "    seed=42,\n",
    "    trajectory_len=cfg.training.trajectory_len,  # Only used when training trajectory model\n",
    ")\n",
    "val_dataloader = tgl.DataLoader(datamodule, 1, shuffle=False, num_workers=0)\n",
    "\n",
    "samples = list(enumerate(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "door_cnt = 0\n",
    "door_samples = []\n",
    "for sample in tqdm.tqdm(samples):\n",
    "    sample_id = sample[1].id[0]\n",
    "    sample_class = id_to_obj_class[sample_id]\n",
    "    if sample_class==\"Door\":\n",
    "        door_cnt += 1\n",
    "        door_samples.append(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "door_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffuser visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
    "from open_anything_diffusion.metrics.trajectory import artflownet_loss, flow_metrics, normalize_trajectory\n",
    "\n",
    "@torch.no_grad()\n",
    "def diffuse_visual(batch, model):  # 1 sample batch\n",
    "    model.eval()\n",
    "    \n",
    "    animation = FlowNetAnimation()\n",
    "    pcd = batch.pos.cpu().numpy()\n",
    "    mask = batch.mask.cpu().long().numpy()\n",
    "\n",
    "    fix_noise = torch.randn_like(batch.delta, device=\"cuda\")\n",
    "\n",
    "    bs = batch.delta.shape[0] // 1200\n",
    "    # batch.traj_noise = torch.randn_like(batch.delta, device=\"cuda\")\n",
    "    batch.traj_noise = fix_noise\n",
    "    # batch.traj_noise = normalize_trajectory(batch.traj_noise)\n",
    "    # breakpoint()\n",
    "\n",
    "    # import time\n",
    "    # batch_time = 0\n",
    "    # model_time = 0\n",
    "    # noise_scheduler_time = 0\n",
    "    # self.noise_scheduler_inference.set_timesteps(self.num_inference_timesteps)\n",
    "    # print(self.noise_scheduler_inference.timesteps)\n",
    "    # for t in self.noise_scheduler_inference.timesteps:\n",
    "    for t in model.noise_scheduler.timesteps:\n",
    "        \n",
    "        # tm = time.time()\n",
    "        batch.timesteps = torch.zeros(bs, device=model.device) + t  # Uniform t steps\n",
    "        batch.timesteps = batch.timesteps.long()\n",
    "        # batch_time += time.time() - tm\n",
    "\n",
    "        # tm = time.time()\n",
    "        model_output = model(batch)          # bs * 1200, traj_len * 3\n",
    "        model_output = model_output.reshape(model_output.shape[0], -1, 3)  # bs * 1200, traj_len, 3\n",
    "        \n",
    "        batch.traj_noise = model.noise_scheduler.step(\n",
    "            # batch.traj_noise = self.noise_scheduler_inference.step(\n",
    "            model_output.reshape(\n",
    "                -1, model.sample_size, model_output.shape[1], model_output.shape[2]\n",
    "            ),\n",
    "            t,\n",
    "            batch.traj_noise.reshape(\n",
    "                -1, model.sample_size, model_output.shape[1], model_output.shape[2]\n",
    "            ),\n",
    "        ).prev_sample\n",
    "        batch.traj_noise = torch.flatten(batch.traj_noise, start_dim=0, end_dim=1)\n",
    "\n",
    "        # print(batch.traj_noise)\n",
    "        if t % 50 == 0:\n",
    "            flow = batch.traj_noise.squeeze().cpu().numpy()\n",
    "            # print(flow[mask])\n",
    "            # segmented_flow = np.zeros_like(flow, dtype=np.float32)\n",
    "            # segmented_flow[mask] = flow[mask]\n",
    "            # print(\"seg\", segmented_flow, \"flow\", flow)\n",
    "            animation.add_trace(\n",
    "                torch.as_tensor(pcd),\n",
    "                # torch.as_tensor([pcd[mask]]),\n",
    "                # torch.as_tensor([flow[mask].detach().cpu().numpy()]),\n",
    "                torch.as_tensor([pcd]),\n",
    "                torch.as_tensor([normalize_trajectory(batch.traj_noise).squeeze().cpu().numpy()]),\n",
    "                \"red\",\n",
    "            )\n",
    "\n",
    "    f_pred = batch.traj_noise\n",
    "    f_pred = normalize_trajectory(f_pred)\n",
    "    # largest_mag: float = torch.linalg.norm(\n",
    "    #     f_pred, ord=2, dim=-1\n",
    "    # ).max()\n",
    "    # f_pred = f_pred / (largest_mag + 1e-6)\n",
    "\n",
    "    # Compute the loss.\n",
    "    n_nodes = torch.as_tensor([d.num_nodes for d in batch.to_data_list()]).to(\"cuda\")  # type: ignore\n",
    "    f_ix = batch.mask.bool()\n",
    "    f_target = batch.delta\n",
    "    f_target = normalize_trajectory(f_target)\n",
    "\n",
    "    f_target = f_target.float()\n",
    "    # loss = artflownet_loss(f_pred, f_target, n_nodes)\n",
    "\n",
    "    # Compute some metrics on flow-only regions.\n",
    "    rmse, cos_dist, mag_error = flow_metrics(\n",
    "        f_pred[f_ix], batch.delta[f_ix]\n",
    "    )\n",
    "\n",
    "    return cos_dist, animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpad.pyg.nets.pointnet2 as pnp\n",
    "from open_anything_diffusion.models.flow_trajectory_diffuser import (\n",
    "    FlowTrajectoryDiffusionModule,\n",
    ")\n",
    "ckpt_path = \"/home/yishu/open_anything_diffusion/logs/train_trajectory/2023-08-31/16-13-10/checkpoints/epoch=394-step=310470-val_loss=0.00-weights-only.ckpt\"\n",
    "network = pnp.PN2Dense(\n",
    "    in_channels=67,\n",
    "    out_channels=3,\n",
    "    p=pnp.PN2DenseParams(),\n",
    ")\n",
    "\n",
    "model = FlowTrajectoryDiffusionModule(network, cfg.training, cfg.model)\n",
    "ckpt = torch.load(ckpt_path)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import math\n",
    "best_animations = []\n",
    "best_cos_dists = []\n",
    "worst_animations = []\n",
    "worst_cos_dists = []\n",
    "mean_cos_dist = 0\n",
    "for sample in tqdm.tqdm(door_samples[1:2]):\n",
    "    best_cos = -1\n",
    "    best_cos_reverse = 1\n",
    "    for repeat in range(10):\n",
    "        cos_dist, animation = diffuse_visual(sample.cuda(), model)\n",
    "        if cos_dist > best_cos:\n",
    "            best_animation = animation\n",
    "        if cos_dist < best_cos_reverse:\n",
    "            worst_animation = animation\n",
    "        \n",
    "        best_cos = max(best_cos, cos_dist)\n",
    "        best_cos_reverse = min(best_cos_reverse, cos_dist)\n",
    "    \n",
    "    best_animations.append(best_animation)\n",
    "    best_cos_dists.append(best_cos)\n",
    "    worst_animations.append(worst_animation)\n",
    "    worst_cos_dists.append(best_cos_reverse)\n",
    "    mean_cos_dist += best_cos\n",
    "mean_cos_dist /= 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cos_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(27):\n",
    "    print(best_cos_dists[i], worst_cos_dists[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = animation[0].animate()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = animation.animate()\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openany",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
