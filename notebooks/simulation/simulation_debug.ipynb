{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pybullet as p\n",
    "import rpad.pyg.nets.pointnet2 as pnp\n",
    "import torch\n",
    "\n",
    "# Trial with dit\n",
    "from open_anything_diffusion.models.modules.dit_models import DiT\n",
    "torch.set_printoptions(precision=10)  # Set higher precision for PyTorch outputs\n",
    "np.set_printoptions(precision=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = DiT(\n",
    "    in_channels=3 + 3,\n",
    "    depth=5,\n",
    "    hidden_size=128,\n",
    "    num_heads=4,\n",
    "    learn_sigma=True,\n",
    ").cuda()\n",
    "\n",
    "# ckpt_file = \"/home/yishu/open_anything_diffusion/logs/train_trajectory_diffuser_dit/2024-03-10/10-54-13/checkpoints/epoch=459-step=80500-val_loss=0.00-weights-only.ckpt\"\n",
    "ckpt_file = \"/home/yishu/open_anything_diffusion/logs/train_trajectory_diffuser_dit/2024-03-30/07-12-41/checkpoints/epoch=359-step=199080-val_loss=0.00-weights-only.ckpt\"\n",
    "from hydra import compose, initialize\n",
    "\n",
    "initialize(config_path=\"../../configs\", version_base=\"1.3\")\n",
    "cfg = compose(config_name=\"eval_sim\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pybullet as p\n",
    "import torch\n",
    "from flowbot3d.datasets.flow_dataset import compute_normalized_flow\n",
    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
    "from rpad.partnet_mobility_utils.data import PMObject\n",
    "from rpad.partnet_mobility_utils.render.pybullet import PMRenderEnv\n",
    "from rpad.pybullet_envs.suction_gripper import FloatingSuctionGripper\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from open_anything_diffusion.datasets.flow_trajectory_dataset import (\n",
    "    compute_flow_trajectory,\n",
    ")\n",
    "from open_anything_diffusion.metrics.trajectory import normalize_trajectory\n",
    "\n",
    "\n",
    "class PMSuctionSim:\n",
    "    def __init__(self, obj_id: str, dataset_path: str, gui: bool = False):\n",
    "        self.render_env = PMRenderEnv(obj_id=obj_id, dataset_path=dataset_path, gui=gui)\n",
    "        self.gui = gui\n",
    "        self.gripper = FloatingSuctionGripper(self.render_env.client_id)\n",
    "        self.gripper.set_pose(\n",
    "            [-1, 0.6, 0.8], p.getQuaternionFromEuler([0, np.pi / 2, 0])\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def reset_gripper(self):\n",
    "        # print(self.gripper.contact_const)\n",
    "        self.gripper.release()\n",
    "        self.gripper.set_pose(\n",
    "            [-1, 0.6, 0.8], p.getQuaternionFromEuler([0, np.pi / 2, 0])\n",
    "        )\n",
    "\n",
    "    def set_gripper_pose(self, pos, ori):\n",
    "        self.gripper.set_pose(pos, ori)\n",
    "\n",
    "    def set_joint_state(self, link_name: str, value: float):\n",
    "        p.resetJointState(\n",
    "            self.render_env.obj_id,\n",
    "            self.render_env.link_name_to_index[link_name],\n",
    "            value,\n",
    "            0.0,\n",
    "            self.render_env.client_id,\n",
    "        )\n",
    "\n",
    "    def render(self, filter_nonobj_pts: bool = False, n_pts: Optional[int] = None):\n",
    "        output = self.render_env.render()\n",
    "        rgb, depth, seg, P_cam, P_world, pc_seg, segmap = output\n",
    "\n",
    "        if filter_nonobj_pts:\n",
    "            pc_seg_obj = np.ones_like(pc_seg) * -1\n",
    "            for k, (body, link) in segmap.items():\n",
    "                if body == self.render_env.obj_id:\n",
    "                    ixs = pc_seg == k\n",
    "                    pc_seg_obj[ixs] = link\n",
    "\n",
    "            is_obj = pc_seg_obj != -1\n",
    "            P_cam = P_cam[is_obj]\n",
    "            P_world = P_world[is_obj]\n",
    "            pc_seg = pc_seg_obj[is_obj]\n",
    "        if n_pts is not None:\n",
    "            perm = np.random.permutation(len(P_world))[:n_pts]\n",
    "            P_cam = P_cam[perm]\n",
    "            P_world = P_world[perm]\n",
    "            pc_seg = pc_seg[perm]\n",
    "\n",
    "        return rgb, depth, seg, P_cam, P_world, pc_seg, segmap\n",
    "\n",
    "    def set_camera(self):\n",
    "        pass\n",
    "\n",
    "    def teleport_and_approach(\n",
    "        self, point, contact_vector, video_writer=None, standoff_d: float = 0.2\n",
    "    ):\n",
    "        # Normalize contact vector.\n",
    "        contact_vector = (contact_vector / contact_vector.norm(dim=-1)).float()\n",
    "\n",
    "        p_teleport = (torch.from_numpy(point) + contact_vector * standoff_d).float()\n",
    "\n",
    "        # breakpoint()\n",
    "\n",
    "        e_z_init = torch.tensor([0, 0, 1.0]).float()\n",
    "        e_y = -contact_vector\n",
    "        e_x = torch.cross(-contact_vector, e_z_init)\n",
    "        e_x = e_x / e_x.norm(dim=-1)\n",
    "        e_z = torch.cross(e_x, e_y)\n",
    "        e_z = e_z / e_z.norm(dim=-1)\n",
    "        R_teleport = torch.stack([e_x, e_y, e_z], dim=1)\n",
    "        R_gripper = torch.as_tensor(\n",
    "            [\n",
    "                [1, 0, 0],\n",
    "                [0, 0, 1.0],\n",
    "                [0, -1.0, 0],\n",
    "            ]\n",
    "        )\n",
    "        # breakpoint()\n",
    "        o_teleport = R.from_matrix(R_teleport @ R_gripper).as_quat()\n",
    "\n",
    "        self.gripper.set_pose(p_teleport, o_teleport)\n",
    "\n",
    "        contact = self.gripper.detect_contact(self.render_env.obj_id)\n",
    "        max_steps = 500\n",
    "        curr_steps = 0\n",
    "        self.gripper.set_velocity(-contact_vector * 0.4, [0, 0, 0])\n",
    "        while not contact and curr_steps < max_steps:\n",
    "            p.stepSimulation(self.render_env.client_id)\n",
    "\n",
    "            if video_writer is not None and curr_steps % 50 == 49:\n",
    "                # if video_writer is not None:\n",
    "                frame_width = 640\n",
    "                frame_height = 480\n",
    "                width, height, rgbImg, depthImg, segImg = p.getCameraImage(\n",
    "                    width=frame_width,\n",
    "                    height=frame_height,\n",
    "                    viewMatrix=p.computeViewMatrixFromYawPitchRoll(\n",
    "                        cameraTargetPosition=[0, 0, 0],\n",
    "                        distance=5,\n",
    "                        # yaw=270,\n",
    "                        # distance=3,\n",
    "                        yaw=90,\n",
    "                        pitch=-30,\n",
    "                        roll=0,\n",
    "                        upAxisIndex=2,\n",
    "                    ),\n",
    "                    projectionMatrix=p.computeProjectionMatrixFOV(\n",
    "                        fov=60,\n",
    "                        aspect=float(frame_width) / frame_height,\n",
    "                        nearVal=0.1,\n",
    "                        farVal=100.0,\n",
    "                    ),\n",
    "                )\n",
    "                image = np.array(rgbImg, dtype=np.uint8)\n",
    "                image = image[:, :, :3]\n",
    "\n",
    "                # Add the frame to the video\n",
    "                video_writer.append_data(image)\n",
    "\n",
    "            curr_steps += 1\n",
    "            if self.gui:\n",
    "                time.sleep(1 / 240.0)\n",
    "            if curr_steps % 1 == 0:\n",
    "                contact = self.gripper.detect_contact(self.render_env.obj_id)\n",
    "\n",
    "        # Give it another chance\n",
    "        if contact:\n",
    "            print(\"contact detected\")\n",
    "\n",
    "        return contact\n",
    "\n",
    "    def teleport(\n",
    "        self, points, contact_vectors, video_writer=None, standoff_d: float = 0.2\n",
    "    ):\n",
    "        # p.setTimeStep(1.0/240)\n",
    "        for id, (point, contact_vector) in enumerate(zip(points, contact_vectors)):\n",
    "            # Normalize contact vector.\n",
    "            # contact_vector = -1 * contact_vector\n",
    "            contact_vector = (contact_vector / contact_vector.norm(dim=-1)).float()\n",
    "            p_teleport = (torch.from_numpy(point) + contact_vector * standoff_d).float()\n",
    "            # breakpoint()\n",
    "\n",
    "            e_z_init = torch.tensor([0, 0, 1.0]).float()\n",
    "            e_y = -contact_vector\n",
    "            e_x = torch.cross(-contact_vector, e_z_init)\n",
    "            e_x = e_x / e_x.norm(dim=-1)\n",
    "            e_z = torch.cross(e_x, e_y)\n",
    "            e_z = e_z / e_z.norm(dim=-1)\n",
    "            R_teleport = torch.stack([e_x, e_y, e_z], dim=1)\n",
    "            R_gripper = torch.as_tensor(\n",
    "                [\n",
    "                    [1, 0, 0],\n",
    "                    [0, 0, 1.0],\n",
    "                    [0, -1.0, 0],\n",
    "                ]\n",
    "            )\n",
    "            o_teleport = R.from_matrix(R_teleport @ R_gripper).as_quat()\n",
    "            self.gripper.set_pose(p_teleport, o_teleport)\n",
    "\n",
    "            contact = self.gripper.detect_contact(self.render_env.obj_id)\n",
    "            max_steps = 500\n",
    "            curr_steps = 0\n",
    "            self.gripper.set_velocity(-contact_vector * 0.4, [0, 0, 0])\n",
    "            while not contact and curr_steps < max_steps:\n",
    "                p.stepSimulation(self.render_env.client_id)\n",
    "                # print(point, p.getBasePositionAndOrientation(self.gripper.body_id),p.getBasePositionAndOrientation(self.gripper.base_id))\n",
    "                if video_writer is not None and curr_steps % 50 == 49:\n",
    "                    # if video_writer is not None:\n",
    "                    frame_width = 640\n",
    "                    frame_height = 480\n",
    "                    width, height, rgbImg, depthImg, segImg = p.getCameraImage(\n",
    "                        width=frame_width,\n",
    "                        height=frame_height,\n",
    "                        viewMatrix=p.computeViewMatrixFromYawPitchRoll(\n",
    "                            cameraTargetPosition=[0, 0, 0],\n",
    "                            distance=5,\n",
    "                            # yaw=270,\n",
    "                            yaw=90,\n",
    "                            pitch=-30,\n",
    "                            roll=0,\n",
    "                            upAxisIndex=2,\n",
    "                        ),\n",
    "                        projectionMatrix=p.computeProjectionMatrixFOV(\n",
    "                            fov=60,\n",
    "                            aspect=float(frame_width) / frame_height,\n",
    "                            nearVal=0.1,\n",
    "                            farVal=100.0,\n",
    "                        ),\n",
    "                    )\n",
    "                    image = np.array(rgbImg, dtype=np.uint8)\n",
    "                    image = image[:, :, :3]\n",
    "\n",
    "                    # Add the frame to the video\n",
    "                    video_writer.append_data(image)\n",
    "\n",
    "                curr_steps += 1\n",
    "                if self.gui:\n",
    "                    time.sleep(1 / 240.0)\n",
    "                if curr_steps % 1 == 0:\n",
    "                    contact = self.gripper.detect_contact(self.render_env.obj_id)\n",
    "\n",
    "            # Give it another chance\n",
    "            if contact:\n",
    "                print(\"contact detected\")\n",
    "                return id, True\n",
    "\n",
    "        return -1, False\n",
    "\n",
    "    def attach(self):\n",
    "        self.gripper.activate(self.render_env.obj_id)\n",
    "\n",
    "    def pull(self, direction, n_steps: int = 100):\n",
    "        direction = torch.as_tensor(direction)\n",
    "        direction = direction / direction.norm(dim=-1)\n",
    "        # breakpoint()\n",
    "        for _ in range(n_steps):\n",
    "            self.gripper.set_velocity(direction * 0.4, [0, 0, 0])\n",
    "            p.stepSimulation(self.render_env.client_id)\n",
    "            if self.gui:\n",
    "                time.sleep(1 / 240.0)\n",
    "\n",
    "    def get_joint_value(self, target_link: str):\n",
    "        link_index = self.render_env.link_name_to_index[target_link]\n",
    "        state = p.getJointState(\n",
    "            self.render_env.obj_id, link_index, self.render_env.client_id\n",
    "        )\n",
    "        joint_pos = state[0]\n",
    "        return joint_pos\n",
    "\n",
    "    def detect_success(self, target_link: str):\n",
    "        link_index = self.render_env.link_name_to_index[target_link]\n",
    "        info = p.getJointInfo(\n",
    "            self.render_env.obj_id, link_index, self.render_env.client_id\n",
    "        )\n",
    "        lower, upper = info[8], info[9]\n",
    "        curr_pos = self.get_joint_value(target_link)\n",
    "\n",
    "        sign = -1 if upper < lower else 1\n",
    "        print(\n",
    "            f\"lower: {lower}, upper: {upper}, curr: {curr_pos}, success:{(upper - curr_pos) / (upper - lower) < 0.1}\"\n",
    "        )\n",
    "\n",
    "        return (upper - curr_pos) / (upper - lower) < 0.1, (curr_pos - lower) / (\n",
    "            upper - lower\n",
    "        )\n",
    "\n",
    "    def randomize_joints(self):\n",
    "        for i in range(\n",
    "            p.getNumJoints(self.render_env.obj_id, self.render_env.client_id)\n",
    "        ):\n",
    "            jinfo = p.getJointInfo(self.render_env.obj_id, i, self.render_env.client_id)\n",
    "            if jinfo[2] == p.JOINT_REVOLUTE or jinfo[2] == p.JOINT_PRISMATIC:\n",
    "                lower, upper = jinfo[8], jinfo[9]\n",
    "                angle = np.random.random() * (upper - lower) + lower\n",
    "                p.resetJointState(\n",
    "                    self.render_env.obj_id, i, angle, 0, self.render_env.client_id\n",
    "                )\n",
    "\n",
    "    def randomize_specific_joints(self, joint_list):\n",
    "        for i in range(\n",
    "            p.getNumJoints(self.render_env.obj_id, self.render_env.client_id)\n",
    "        ):\n",
    "            jinfo = p.getJointInfo(self.render_env.obj_id, i, self.render_env.client_id)\n",
    "            if jinfo[12].decode(\"UTF-8\") in joint_list:\n",
    "                lower, upper = jinfo[8], jinfo[9]\n",
    "                angle = np.random.random() * (upper - lower) + lower\n",
    "                p.resetJointState(\n",
    "                    self.render_env.obj_id, i, angle, 0, self.render_env.client_id\n",
    "                )\n",
    "\n",
    "    def articulate_specific_joints(self, joint_list, amount):\n",
    "        for i in range(\n",
    "            p.getNumJoints(self.render_env.obj_id, self.render_env.client_id)\n",
    "        ):\n",
    "            jinfo = p.getJointInfo(self.render_env.obj_id, i, self.render_env.client_id)\n",
    "            if jinfo[12].decode(\"UTF-8\") in joint_list:\n",
    "                lower, upper = jinfo[8], jinfo[9]\n",
    "                angle = amount * (upper - lower) + lower\n",
    "                p.resetJointState(\n",
    "                    self.render_env.obj_id, i, angle, 0, self.render_env.client_id\n",
    "                )\n",
    "\n",
    "    def randomize_joints_openclose(self, joint_list):\n",
    "        randind = np.random.choice([0, 1])\n",
    "        # Close: 0\n",
    "        # Open: 1\n",
    "        self.close_or_open = randind\n",
    "        for i in range(\n",
    "            p.getNumJoints(self.render_env.obj_id, self.render_env.client_id)\n",
    "        ):\n",
    "            jinfo = p.getJointInfo(self.render_env.obj_id, i, self.render_env.client_id)\n",
    "            if jinfo[12].decode(\"UTF-8\") in joint_list:\n",
    "                lower, upper = jinfo[8], jinfo[9]\n",
    "                angles = [lower, upper]\n",
    "                angle = angles[randind]\n",
    "                p.resetJointState(\n",
    "                    self.render_env.obj_id, i, angle, 0, self.render_env.client_id\n",
    "                )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrialResult:\n",
    "    success: bool\n",
    "    contact: bool\n",
    "    assertion: bool\n",
    "    init_angle: float\n",
    "    final_angle: float\n",
    "    now_angle: float\n",
    "\n",
    "    # UMPNet metric goes here\n",
    "    metric: float\n",
    "\n",
    "\n",
    "class GTFlowModel:\n",
    "    def __init__(self, raw_data, env):\n",
    "        self.env = env\n",
    "        self.raw_data = raw_data\n",
    "\n",
    "    def __call__(self, obs) -> torch.Tensor:\n",
    "        rgb, depth, seg, P_cam, P_world, pc_seg, segmap = obs\n",
    "        env = self.env\n",
    "        raw_data = self.raw_data\n",
    "\n",
    "        links = raw_data.semantics.by_type(\"slider\")\n",
    "        links += raw_data.semantics.by_type(\"hinge\")\n",
    "        current_jas = {}\n",
    "        for link in links:\n",
    "            linkname = link.name\n",
    "            chain = raw_data.obj.get_chain(linkname)\n",
    "            for joint in chain:\n",
    "                current_jas[joint.name] = 0\n",
    "\n",
    "        normalized_flow = compute_normalized_flow(\n",
    "            P_world,\n",
    "            env.render_env.T_world_base,\n",
    "            current_jas,\n",
    "            pc_seg,\n",
    "            env.render_env.link_name_to_index,\n",
    "            raw_data,\n",
    "            \"all\",\n",
    "        )\n",
    "\n",
    "        return torch.from_numpy(normalized_flow)\n",
    "\n",
    "    def get_movable_mask(self, obs) -> torch.Tensor:\n",
    "        flow = self(obs)\n",
    "        mask = (~(np.isclose(flow, 0.0)).all(axis=-1)).astype(np.bool_)\n",
    "        return mask\n",
    "\n",
    "\n",
    "class GTTrajectoryModel:\n",
    "    def __init__(self, raw_data, env, traj_len=20):\n",
    "        self.raw_data = raw_data\n",
    "        self.env = env\n",
    "        self.traj_len = traj_len\n",
    "\n",
    "    def __call__(self, obs) -> torch.Tensor:\n",
    "        rgb, depth, seg, P_cam, P_world, pc_seg, segmap = obs\n",
    "        env = self.env\n",
    "        raw_data = self.raw_data\n",
    "\n",
    "        links = raw_data.semantics.by_type(\"slider\")\n",
    "        links += raw_data.semantics.by_type(\"hinge\")\n",
    "        current_jas = {}\n",
    "        for link in links:\n",
    "            linkname = link.name\n",
    "            chain = raw_data.obj.get_chain(linkname)\n",
    "            for joint in chain:\n",
    "                current_jas[joint.name] = 0\n",
    "        trajectory, _ = compute_flow_trajectory(\n",
    "            self.traj_len,\n",
    "            P_world,\n",
    "            env.render_env.T_world_base,\n",
    "            current_jas,\n",
    "            pc_seg,\n",
    "            env.render_env.link_name_to_index,\n",
    "            raw_data,\n",
    "            \"all\",\n",
    "        )\n",
    "        return torch.from_numpy(trajectory)\n",
    "\n",
    "\n",
    "def choose_grasp_points(raw_pred_flow, raw_point_cloud, filter_edge=False, k=20):\n",
    "    pred_flow = raw_pred_flow.clone()\n",
    "    point_cloud = raw_point_cloud\n",
    "    # Choose top k non-edge grasp points:\n",
    "    if filter_edge:  # Need to filter the edge points\n",
    "        squared_diff = (\n",
    "            point_cloud[:, np.newaxis, :] - point_cloud[np.newaxis, :, :]\n",
    "        ) ** 2\n",
    "        dists = np.sqrt(np.sum(squared_diff, axis=2))\n",
    "        dist_thres = np.percentile(dists, 10)\n",
    "        neighbour_points = np.sum(dists < dist_thres, axis=0)\n",
    "        invalid_points = neighbour_points < np.percentile(\n",
    "            neighbour_points, 30\n",
    "        )  # Not edge\n",
    "        pred_flow[invalid_points] = 0  # Don't choose these edge points!!!!!\n",
    "\n",
    "    top_k_point = min(k, len(pred_flow))\n",
    "    best_flow_ix = torch.topk(pred_flow.norm(dim=-1), top_k_point)[1]\n",
    "    if top_k_point == 1:\n",
    "        best_flow_ix = torch.tensor(list(best_flow_ix) * 2)\n",
    "    best_flow = pred_flow[best_flow_ix]\n",
    "    best_point = point_cloud[best_flow_ix]\n",
    "    return best_flow_ix, best_flow, best_point\n",
    "\n",
    "\n",
    "def run_trial(\n",
    "    env: PMSuctionSim,\n",
    "    raw_data: PMObject,\n",
    "    target_link: str,\n",
    "    model,\n",
    "    gt_model=None,  # When we use mask_input_channel=True, this is the mask generator\n",
    "    n_steps: int = 30,\n",
    "    n_pts: int = 1200,\n",
    "    save_name: str = \"unknown\",\n",
    "    website: bool = False,\n",
    "    gui: bool = False,\n",
    ") -> TrialResult:\n",
    "    torch.manual_seed(42)\n",
    "    torch.set_printoptions(precision=10)  # Set higher precision for PyTorch outputs\n",
    "    np.set_printoptions(precision=10)\n",
    "    # p.setPhysicsEngineParameter(numSolverIterations=10)\n",
    "    # p.setPhysicsEngineParameter(contactBreakingThreshold=0.01, contactSlop=0.001)\n",
    "\n",
    "    sim_trajectory = [0.05] + [0] * (n_steps)  # start from 0.05\n",
    "\n",
    "    if website:\n",
    "        # Flow animation\n",
    "        animation = FlowNetAnimation()\n",
    "\n",
    "    # First, reset the environment.\n",
    "    env.reset()\n",
    "    # Joint information\n",
    "    info = p.getJointInfo(\n",
    "        env.render_env.obj_id,\n",
    "        env.render_env.link_name_to_index[target_link],\n",
    "        env.render_env.client_id,\n",
    "    )\n",
    "    init_angle, target_angle = info[8], info[9]\n",
    "\n",
    "    # Sometimes doors collide with themselves. It's dumb.\n",
    "    if (\n",
    "        raw_data.category == \"Door\"\n",
    "        and raw_data.semantics.by_name(target_link).type == \"hinge\"\n",
    "    ):\n",
    "        env.set_joint_state(\n",
    "            target_link, init_angle + 0.05 * (target_angle - init_angle)\n",
    "        )\n",
    "        # env.set_joint_state(target_link, 0.2)\n",
    "\n",
    "    if raw_data.semantics.by_name(target_link).type == \"hinge\":\n",
    "        env.set_joint_state(\n",
    "            target_link, init_angle + 0.05 * (target_angle - init_angle)\n",
    "        )\n",
    "        # env.set_joint_state(target_link, 0.05)\n",
    "\n",
    "    # Predict the flow on the observation.\n",
    "    pc_obs = env.render(filter_nonobj_pts=True, n_pts=n_pts)\n",
    "    rgb, depth, seg, P_cam, P_world, pc_seg, segmap = pc_obs\n",
    "\n",
    "    if init_angle == target_angle:  # Not movable\n",
    "        p.disconnect(physicsClientId=env.render_env.client_id)\n",
    "        return (\n",
    "            None,\n",
    "            TrialResult(\n",
    "                success=False,\n",
    "                assertion=False,\n",
    "                contact=False,\n",
    "                init_angle=0,\n",
    "                final_angle=0,\n",
    "                now_angle=0,\n",
    "                metric=0,\n",
    "            ),\n",
    "            sim_trajectory,\n",
    "        )\n",
    "\n",
    "    # breakpoint()\n",
    "    if gt_model is None:  # GT Flow model\n",
    "        pred_trajectory = model(copy.deepcopy(pc_obs))\n",
    "    else:\n",
    "        movable_mask = gt_model.get_movable_mask(pc_obs)\n",
    "        pred_trajectory = model(copy.deepcopy(pc_obs), movable_mask)\n",
    "    # pred_trajectory = model(copy.deepcopy(pc_obs))\n",
    "    # breakpoint()\n",
    "    pred_trajectory = pred_trajectory.reshape(\n",
    "        pred_trajectory.shape[0], -1, pred_trajectory.shape[-1]\n",
    "    )\n",
    "    traj_len = pred_trajectory.shape[1]  # Trajectory length\n",
    "    print(f\"Predicting {traj_len} length trajectories.\")\n",
    "    pred_flow = pred_trajectory[:, 0, :]\n",
    "\n",
    "    # flow_fig(torch.from_numpy(P_world), pred_flow, sizeref=0.1, use_v2=True).show()\n",
    "    # breakpoint()\n",
    "\n",
    "    # Filter down just the points on the target link.\n",
    "    link_ixs = pc_seg == env.render_env.link_name_to_index[target_link]\n",
    "    # assert link_ixs.any()\n",
    "    if not link_ixs.any():\n",
    "        p.disconnect(physicsClientId=env.render_env.client_id)\n",
    "        print(\"link_ixs finds no point\")\n",
    "        animation_results = animation.animate() if website else None\n",
    "        return (\n",
    "            animation_results,\n",
    "            TrialResult(\n",
    "                success=False,\n",
    "                assertion=False,\n",
    "                contact=False,\n",
    "                init_angle=0,\n",
    "                final_angle=0,\n",
    "                now_angle=0,\n",
    "                metric=0,\n",
    "            ),\n",
    "            sim_trajectory,\n",
    "        )\n",
    "\n",
    "    if website:\n",
    "        if gui:\n",
    "            # Record simulation video\n",
    "            log_id = p.startStateLogging(\n",
    "                p.STATE_LOGGING_VIDEO_MP4,\n",
    "                f\"./logs/simu_eval/video_assets/{save_name}.mp4\",\n",
    "            )\n",
    "        else:\n",
    "            video_file = f\"./logs/simu_eval/video_assets/{save_name}.mp4\"\n",
    "            # # cv2 output videos won't show on website\n",
    "            frame_width = 640\n",
    "            frame_height = 480\n",
    "            # fps = 5\n",
    "            # fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            # videoWriter = cv2.VideoWriter(video_file, fourcc, fps, (frame_width, frame_height))\n",
    "            # videoWriter.write(rgbImgOpenCV)\n",
    "\n",
    "            # Camera param\n",
    "            writer = imageio.get_writer(video_file, fps=5)\n",
    "\n",
    "            # Capture image\n",
    "            width, height, rgbImg, depthImg, segImg = p.getCameraImage(\n",
    "                width=frame_width,\n",
    "                height=frame_height,\n",
    "                viewMatrix=p.computeViewMatrixFromYawPitchRoll(\n",
    "                    cameraTargetPosition=[0, 0, 0],\n",
    "                    distance=5,\n",
    "                    # yaw=270, \n",
    "                    yaw = 90,\n",
    "                    pitch=-30,\n",
    "                    roll=0,\n",
    "                    upAxisIndex=2,\n",
    "                ),\n",
    "                projectionMatrix=p.computeProjectionMatrixFOV(\n",
    "                    fov=60,\n",
    "                    aspect=float(frame_width) / frame_height,\n",
    "                    nearVal=0.1,\n",
    "                    farVal=100.0,\n",
    "                ),\n",
    "            )\n",
    "            image = np.array(rgbImg, dtype=np.uint8)\n",
    "            image = image[:, :, :3]\n",
    "\n",
    "            # Add the frame to the video\n",
    "            writer.append_data(image)\n",
    "\n",
    "    # The attachment point is the point with the highest flow.\n",
    "    # best_flow_ix = pred_flow[link_ixs].norm(dim=-1).argmax()\n",
    "    best_flow_ix, best_flows, best_points = choose_grasp_points(\n",
    "        pred_flow[link_ixs], P_world[link_ixs], filter_edge=False, k=20\n",
    "    )\n",
    "\n",
    "    # Teleport to an approach pose, approach, the object and grasp.\n",
    "    if website and not gui:\n",
    "        # contact = env.teleport_and_approach(best_point, best_flow, video_writer=writer)\n",
    "        best_flow_ix, contact = env.teleport(\n",
    "            best_points, best_flows, video_writer=writer\n",
    "        )\n",
    "    else:\n",
    "        # contact = env.teleport_and_approach(best_point, best_flow)\n",
    "        best_flow_ix, contact = env.teleport(best_points, best_flows)\n",
    "    best_flow = pred_flow[link_ixs][best_flow_ix]\n",
    "    best_point = P_world[link_ixs][best_flow_ix]\n",
    "    last_step_grasp_point = best_point\n",
    "\n",
    "    if not contact:\n",
    "        if website:\n",
    "            segmented_flow = np.zeros_like(pred_flow)\n",
    "            # segmented_flow[link_ixs] = pred_flow[link_ixs]\n",
    "            # segmented_flow = np.array(\n",
    "            #     normalize_trajectory(\n",
    "            #         torch.from_numpy(np.expand_dims(segmented_flow, 1))\n",
    "            #     ).squeeze()\n",
    "            # )\n",
    "            point = best_point\n",
    "            contact_vector = best_flow\n",
    "            contact_vector = (contact_vector / contact_vector.norm(dim=-1)).float()\n",
    "            p_teleport = (torch.from_numpy(point) + contact_vector * 0.2).float()\n",
    "            print(segmented_flow[link_ixs].shape)\n",
    "            segmented_flow[link_ixs][best_flow_ix] = p_teleport - point\n",
    "            animation.add_trace(\n",
    "                torch.as_tensor(P_world),\n",
    "                torch.as_tensor([P_world]),\n",
    "                torch.as_tensor([segmented_flow]),\n",
    "                \"red\",\n",
    "            )\n",
    "            if gui:\n",
    "                p.stopStateLogging(log_id)\n",
    "            else:\n",
    "                # Write video\n",
    "                writer.close()\n",
    "                # videoWriter.release()\n",
    "\n",
    "        print(\"No contact!\")\n",
    "        p.disconnect(physicsClientId=env.render_env.client_id)\n",
    "        animation_results = None if not website else animation.animate()\n",
    "        return (\n",
    "            animation_results,\n",
    "            TrialResult(\n",
    "                success=False,\n",
    "                assertion=True,\n",
    "                contact=False,\n",
    "                init_angle=0,\n",
    "                final_angle=0,\n",
    "                now_angle=0,\n",
    "                metric=0,\n",
    "            ),\n",
    "            sim_trajectory,\n",
    "        )\n",
    "\n",
    "    env.attach()\n",
    "\n",
    "    pc_obs = env.render(filter_nonobj_pts=True, n_pts=n_pts)\n",
    "    success = False\n",
    "\n",
    "    global_step = 0\n",
    "    # for i in range(n_steps):\n",
    "    while global_step < n_steps:\n",
    "        # Predict the flow on the observation.\n",
    "        if gt_model is None:  # GT Flow model\n",
    "            pred_trajectory = model(copy.deepcopy(pc_obs))\n",
    "        else:\n",
    "            movable_mask = gt_model.get_movable_mask(pc_obs)\n",
    "            # breakpoint()\n",
    "            pred_trajectory = model(pc_obs, movable_mask)\n",
    "            # pred_trajectory = model(pc_obs)\n",
    "        pred_trajectory = pred_trajectory.reshape(\n",
    "            pred_trajectory.shape[0], -1, pred_trajectory.shape[-1]\n",
    "        )\n",
    "\n",
    "        for traj_step in range(pred_trajectory.shape[1]):\n",
    "            if global_step == n_steps:\n",
    "                break\n",
    "            global_step += 1\n",
    "            pred_flow = pred_trajectory[:, traj_step, :]\n",
    "            rgb, depth, seg, P_cam, P_world, pc_seg, segmap = pc_obs\n",
    "\n",
    "            # Filter down just the points on the target link.\n",
    "            # breakpoint()\n",
    "            link_ixs = pc_seg == env.render_env.link_name_to_index[target_link]\n",
    "            # assert link_ixs.any()\n",
    "            if not link_ixs.any():\n",
    "                if website:\n",
    "                    if gui:\n",
    "                        p.stopStateLogging(log_id)\n",
    "                    else:\n",
    "                        writer.close()\n",
    "                        # videoWriter.release()\n",
    "                p.disconnect(physicsClientId=env.render_env.client_id)\n",
    "                print(\"link_ixs finds no point\")\n",
    "                animation_results = animation.animate() if website else None\n",
    "                return (\n",
    "                    animation_results,\n",
    "                    TrialResult(\n",
    "                        assertion=False,\n",
    "                        success=False,\n",
    "                        contact=False,\n",
    "                        init_angle=0,\n",
    "                        final_angle=0,\n",
    "                        now_angle=0,\n",
    "                        metric=0,\n",
    "                    ),\n",
    "                    sim_trajectory,\n",
    "                )\n",
    "\n",
    "            # Get the best direction.\n",
    "            # best_flow_ix = pred_flow[link_ixs].norm(dim=-1).argmax()\n",
    "            best_flow_ix, best_flows, best_points = choose_grasp_points(\n",
    "                pred_flow[link_ixs], P_world[link_ixs], filter_edge=False, k=20\n",
    "            )\n",
    "\n",
    "            # (1) Strategy 1 - Don't change grasp point\n",
    "            # (2) Strategy 2 - Change grasp point when leverage difference is large\n",
    "            lev_diff_thres = 0.2\n",
    "            no_movement_thres = -1\n",
    "\n",
    "            # # Don't use this policy\n",
    "            # lev_diff_thres = 100\n",
    "            # no_movement_thres = -1\n",
    "            # good_movement_thres = 1000\n",
    "\n",
    "            # Only change if the new point's leverage is a great increase\n",
    "            # gripper_tip_pos = p.getClosestPoints(\n",
    "            #     env.gripper.body_id, env.render_env.obj_id, distance=0.5, linkIndexA=0\n",
    "            # )[0][5]\n",
    "            # gripper_object_contact = p.getContactPoints(\n",
    "            #     env.gripper.body_id, env.render_env.obj_id, linkIndexA=0\n",
    "            # )[0]\n",
    "            # gripper_contact, object_contact = gripper_object_contact[5], gripper_object_contact[6]\n",
    "            gripper_tip_pos, _ = p.getBasePositionAndOrientation(env.gripper.body_id)\n",
    "            pcd_dist = torch.tensor(P_world[link_ixs] - np.array(gripper_tip_pos)).norm(\n",
    "                dim=-1\n",
    "            )\n",
    "            grasp_point_id = pcd_dist.argmin()\n",
    "            lev_diff = best_flows.norm(dim=-1) - pred_flow[link_ixs][\n",
    "                grasp_point_id\n",
    "            ].norm(dim=-1)\n",
    "\n",
    "            gripper_movement = torch.from_numpy(P_world[grasp_point_id] - last_step_grasp_point).norm()\n",
    "            # print(\"gripper: \",gripper_movement)\n",
    "            # breakpoint()\n",
    "            if (\n",
    "                gripper_movement < no_movement_thres\n",
    "                or lev_diff[0] > lev_diff_thres\n",
    "            ):  # pcd_dist < 0.05 -> didn't move much....\n",
    "                env.reset_gripper()\n",
    "                p.stepSimulation(\n",
    "                    env.render_env.client_id\n",
    "                )  # Make sure the constraint is lifted\n",
    "\n",
    "                if website and not gui:\n",
    "                    # contact = env.teleport_and_approach(best_point, best_flow, video_writer=writer)\n",
    "                    best_flow_ix, contact = env.teleport(\n",
    "                        best_points, best_flows, video_writer=writer\n",
    "                    )\n",
    "                else:\n",
    "                    # contact = env.teleport_and_approach(best_point, best_flow)\n",
    "                    best_flow_ix, contact = env.teleport(best_points, best_flows)\n",
    "                best_flow = pred_flow[link_ixs][best_flow_ix]\n",
    "                best_point = P_world[link_ixs][best_flow_ix]\n",
    "                last_step_grasp_point = best_point  # Grasp a new point\n",
    "                # print(\"new!\", last_step_grasp_point)\n",
    "\n",
    "                if not contact:\n",
    "                    if website:\n",
    "                        segmented_flow = np.zeros_like(pred_flow)\n",
    "                        # segmented_flow[link_ixs] = pred_flow[link_ixs]\n",
    "                        # segmented_flow = np.array(\n",
    "                        #     normalize_trajectory(\n",
    "                        #         torch.from_numpy(np.expand_dims(segmented_flow, 1))\n",
    "                        #     ).squeeze()\n",
    "                        # )\n",
    "                        point = best_point\n",
    "                        contact_vector = best_flow\n",
    "                        contact_vector = (contact_vector / contact_vector.norm(dim=-1)).float()\n",
    "                        p_teleport = (torch.from_numpy(point) + contact_vector * 0.2).float()\n",
    "                        print(segmented_flow[link_ixs].shape)\n",
    "                        segmented_flow[link_ixs][best_flow_ix] = p_teleport - point\n",
    "                        print(\"HI!!!:\", p_teleport - point, segmented_flow[link_ixs][best_flow_ix])\n",
    "                        animation.add_trace(\n",
    "                            torch.as_tensor(P_world),\n",
    "                            torch.as_tensor([P_world]),\n",
    "                            torch.as_tensor([segmented_flow]),\n",
    "                            \"red\",\n",
    "                        )\n",
    "                        if gui:\n",
    "                            p.stopStateLogging(log_id)\n",
    "                        else:\n",
    "                            # Write video\n",
    "                            writer.close()\n",
    "                            # videoWriter.release()\n",
    "\n",
    "                    print(\"No contact!\")\n",
    "                    p.disconnect(physicsClientId=env.render_env.client_id)\n",
    "                    animation_results = None if not website else animation.animate()\n",
    "                    return (\n",
    "                        animation_results,\n",
    "                        TrialResult(\n",
    "                            success=False,\n",
    "                            assertion=True,\n",
    "                            contact=False,\n",
    "                            init_angle=0,\n",
    "                            final_angle=0,\n",
    "                            now_angle=0,\n",
    "                            metric=0,\n",
    "                        ),\n",
    "                        sim_trajectory,\n",
    "                    )\n",
    "\n",
    "                env.attach()\n",
    "            else:\n",
    "                best_flow = pred_flow[link_ixs][best_flow_ix[0]]\n",
    "                last_step_grasp_point = P_world[link_ixs][\n",
    "                    grasp_point_id\n",
    "                ]  # The original point - don't need to change\n",
    "                best_point = P_world[link_ixs][grasp_point_id]\n",
    "                best_flow_ix = grasp_point_id\n",
    "                # print(\"same:\", last_step_grasp_point)\n",
    "\n",
    "            env.attach()\n",
    "            # Perform the pulling.\n",
    "            # if best_flow.sum() == 0:\n",
    "            #     continue\n",
    "            # print(best_flow)\n",
    "            env.pull(best_flow)\n",
    "            env.attach()\n",
    "\n",
    "            if website:\n",
    "                # Add pcd to flow animation\n",
    "                segmented_flow = np.zeros_like(pred_flow)\n",
    "                # segmented_flow[link_ixs] = pred_flow[link_ixs]\n",
    "                # segmented_flow = np.array(\n",
    "                #     normalize_trajectory(\n",
    "                #         torch.from_numpy(np.expand_dims(segmented_flow, 1))\n",
    "                #     ).squeeze()\n",
    "                # )\n",
    "                point = best_point\n",
    "                contact_vector = best_flow\n",
    "                contact_vector = (contact_vector / contact_vector.norm(dim=-1)).float()\n",
    "                p_teleport = (torch.from_numpy(point) + contact_vector * 0.2).float()\n",
    "                print(segmented_flow[link_ixs].shape)\n",
    "                segmented_flow[link_ixs][best_flow_ix] = p_teleport - point\n",
    "                print(\"HI!!!:\", p_teleport - point, segmented_flow[link_ixs][best_flow_ix])\n",
    "                animation.add_trace(\n",
    "                    torch.as_tensor(P_world),\n",
    "                    torch.as_tensor([P_world]),\n",
    "                    torch.as_tensor([segmented_flow]),\n",
    "                    \"red\",\n",
    "                )\n",
    "\n",
    "                # Capture frame\n",
    "                width, height, rgbImg, depthImg, segImg = p.getCameraImage(\n",
    "                    width=frame_width,\n",
    "                    height=frame_height,\n",
    "                    viewMatrix=p.computeViewMatrixFromYawPitchRoll(\n",
    "                        cameraTargetPosition=[0, 0, 0],\n",
    "                        distance=5,\n",
    "                        # yaw=270, \n",
    "                        yaw=90, \n",
    "                        pitch=-30,\n",
    "                        roll=0,\n",
    "                        upAxisIndex=2,\n",
    "                    ),\n",
    "                    projectionMatrix=p.computeProjectionMatrixFOV(\n",
    "                        fov=60,\n",
    "                        aspect=float(frame_width) / frame_height,\n",
    "                        nearVal=0.1,\n",
    "                        farVal=100.0,\n",
    "                    ),\n",
    "                )\n",
    "                # rgbImgOpenCV = cv2.cvtColor(np.array(rgbImg), cv2.COLOR_RGB2BGR)\n",
    "                # videoWriter.write(rgbImgOpenCV)\n",
    "                image = np.array(rgbImg, dtype=np.uint8)\n",
    "                image = image[:, :, :3]\n",
    "\n",
    "                # Add the frame to the video\n",
    "                writer.append_data(image)\n",
    "\n",
    "            success, sim_trajectory[global_step] = env.detect_success(target_link)\n",
    "\n",
    "            if success:\n",
    "                for left_step in range(global_step, 31):\n",
    "                    sim_trajectory[left_step] = sim_trajectory[global_step]\n",
    "                break\n",
    "\n",
    "            pc_obs = env.render(filter_nonobj_pts=True, n_pts=1200)\n",
    "\n",
    "        if success:\n",
    "            for left_step in range(global_step, 31):\n",
    "                sim_trajectory[left_step] = sim_trajectory[global_step]\n",
    "            break\n",
    "\n",
    "    # calculate the metrics\n",
    "    curr_pos = env.get_joint_value(target_link)\n",
    "    metric = (curr_pos - init_angle) / (target_angle - init_angle)\n",
    "    metric = min(max(metric, 0), 1)\n",
    "\n",
    "    if website:\n",
    "        if gui:\n",
    "            p.stopStateLogging(log_id)\n",
    "        else:\n",
    "            writer.close()\n",
    "            # videoWriter.release()\n",
    "\n",
    "    p.disconnect(physicsClientId=env.render_env.client_id)\n",
    "    animation_results = None if not website else animation.animate()\n",
    "    return (\n",
    "        animation_results,\n",
    "        TrialResult(  # Save the flow visuals\n",
    "            success=success,\n",
    "            contact=True,\n",
    "            assertion=True,\n",
    "            init_angle=init_angle,\n",
    "            final_angle=target_angle,\n",
    "            now_angle=curr_pos,\n",
    "            metric=metric,\n",
    "        ),\n",
    "        sim_trajectory,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial with diffuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpad.partnet_mobility_utils.data import PMObject\n",
    "from open_anything_diffusion.simulations.suction import PMSuctionSim\n",
    "def trial_with_diffuser(\n",
    "    obj_id=\"41083\",\n",
    "    model=None,\n",
    "    n_step=30,\n",
    "    gui=False,\n",
    "    all_joint=False,\n",
    "    website=False,\n",
    "    available_joints=None,\n",
    "):\n",
    "    # pm_dir = os.path.expanduser(\"~/datasets/partnet-mobility/raw\")\n",
    "    pm_dir = os.path.expanduser(\"~/datasets/partnet-mobility/convex\")\n",
    "    # env = PMSuctionSim(obj_id, pm_dir, gui=gui)\n",
    "    raw_data = PMObject(os.path.join(pm_dir, obj_id))\n",
    "\n",
    "    if available_joints is None:  # Use the passed in joint sets\n",
    "        available_joints = raw_data.semantics.by_type(\n",
    "            \"hinge\"\n",
    "        ) + raw_data.semantics.by_type(\"slider\")\n",
    "        available_joints = [joint.name for joint in available_joints]\n",
    "\n",
    "    print(\"available_joints:\", available_joints)\n",
    "    if all_joint:  # Need to traverse all the joints\n",
    "        picked_joints = available_joints\n",
    "    else:\n",
    "        picked_joints = [available_joints[np.random.randint(0, len(available_joints))]]\n",
    "\n",
    "    sim_trajectories = []\n",
    "    results = []\n",
    "    figs = {}\n",
    "    for joint_name in picked_joints:\n",
    "        # t0 = time.perf_counter()\n",
    "        # print(f\"opening {joint.name}, {joint.label}\")\n",
    "        print(f\"opening {joint_name}\")\n",
    "        env = PMSuctionSim(obj_id, pm_dir, gui=gui)\n",
    "\n",
    "        # Close all joints:\n",
    "        for link_to_restore in [\n",
    "            joint.name\n",
    "            for joint in raw_data.semantics.by_type(\"hinge\")\n",
    "            + raw_data.semantics.by_type(\"slider\")\n",
    "        ]:\n",
    "            info = p.getJointInfo(\n",
    "                env.render_env.obj_id,\n",
    "                env.render_env.link_name_to_index[link_to_restore],\n",
    "                env.render_env.client_id,\n",
    "            )\n",
    "            init_angle, target_angle = info[8], info[9]\n",
    "            env.set_joint_state(link_to_restore, init_angle)\n",
    "\n",
    "        # gt_model = GTFlowModel(raw_data, env)\n",
    "        fig, result, sim_trajectory = run_trial(\n",
    "            env,\n",
    "            raw_data,\n",
    "            joint_name,\n",
    "            model,\n",
    "            gt_model=None,  # Don't need mask\n",
    "            n_steps=n_step,\n",
    "            save_name=f\"{obj_id}_{joint_name}\",\n",
    "            website=website,\n",
    "            gui=gui,\n",
    "        )\n",
    "        sim_trajectories.append(sim_trajectory)\n",
    "        if result.assertion is False:\n",
    "            with open(\n",
    "                \"/home/yishu/open_anything_diffusion/logs/assertion_failure.txt\", \"a\"\n",
    "            ) as f:\n",
    "                f.write(f\"Object: {obj_id}; Joint: {joint_name}\\n\")\n",
    "            continue\n",
    "        if result.contact is False:\n",
    "            continue\n",
    "        figs[joint_name] = fig\n",
    "        results.append(result)\n",
    "\n",
    "    return figs, results, sim_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_anything_diffusion.models.flow_diffuser_dit import (\n",
    "    FlowTrajectoryDiffuserSimulationModule_DiT,\n",
    ")\n",
    "\n",
    "model = FlowTrajectoryDiffuserSimulationModule_DiT(\n",
    "    network, inference_cfg=cfg.inference, model_cfg=cfg.model\n",
    ").cuda()\n",
    "model.load_from_ckpt(ckpt_file)\n",
    "model.eval()\n",
    "\n",
    "trial_figs, trial_results, sim_trajectory = trial_with_diffuser(\n",
    "    obj_id=\"102663\",\n",
    "    model=model,\n",
    "    n_step=5,\n",
    "    gui=False,\n",
    "    website=cfg.website,\n",
    "    all_joint=False,\n",
    "    available_joints=[\"link_2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_figs['link_2'].show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openany",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
