{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "initialize(config_path=\"../../configs\", version_base=\"1.3\")\n",
    "cfg = compose(config_name=\"eval_sim\")\n",
    "\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import pybullet as p\n",
    "import torch\n",
    "import tqdm\n",
    "from rpad.partnet_mobility_utils.data import PMObject\n",
    "\n",
    "from open_anything_diffusion.metrics.trajectory import flow_metrics\n",
    "from open_anything_diffusion.models.flow_diffuser_hispndit import (\n",
    "    FlowTrajectoryDiffuserSimulationModule_HisPNDiT,\n",
    ")\n",
    "from open_anything_diffusion.models.modules.dit_models import PN2HisDiT\n",
    "from open_anything_diffusion.models.modules.history_encoder import HistoryEncoder\n",
    "from open_anything_diffusion.simulations.suction import GTFlowModel, PMSuctionSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = {\n",
    "    \"DiT\": PN2HisDiT(\n",
    "        history_embed_dim=128,\n",
    "        in_channels=3,\n",
    "        depth=5,\n",
    "        hidden_size=128,\n",
    "        num_heads=4,\n",
    "        # depth=8,\n",
    "        # hidden_size=256,\n",
    "        # num_heads=4,\n",
    "        learn_sigma=True,\n",
    "    ).cuda(),\n",
    "    \"History\": HistoryEncoder(\n",
    "        history_dim=128,\n",
    "        history_len=1,\n",
    "        batch_norm=True,\n",
    "        transformer=False,\n",
    "        repeat_dim=False,\n",
    "    ).cuda(),\n",
    "}\n",
    "ckpt_file = \"/home/yishu/open_anything_diffusion/logs/train_trajectory_diffuser_hispndit/2024-05-25/02-00-54/checkpoints/epoch=399-step=331600-val_loss=0.00-weights-only.ckpt\"\n",
    "model = FlowTrajectoryDiffuserSimulationModule_HisPNDiT(\n",
    "    network, inference_cfg=cfg.inference, model_cfg=cfg.model\n",
    ").cuda()\n",
    "model.load_from_ckpt(ckpt_file)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_id = \"8877\"\n",
    "joint_id = 0\n",
    "pm_dir = os.path.expanduser(\"~/datasets/partnet-mobility/convex\")\n",
    "\n",
    "raw_data = PMObject(os.path.join(pm_dir, obj_id))\n",
    "available_joints = raw_data.semantics.by_type(\"hinge\") + raw_data.semantics.by_type(\n",
    "    \"slider\"\n",
    ")\n",
    "available_joints = [joint.name for joint in available_joints]\n",
    "\n",
    "env = PMSuctionSim(obj_id, pm_dir, gui=False)\n",
    "gt_model = GTFlowModel(raw_data, env)\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for joint in available_joints:  # Close all joints\n",
    "    info = p.getJointInfo(\n",
    "        env.render_env.obj_id,\n",
    "        env.render_env.link_name_to_index[joint],\n",
    "        env.render_env.client_id,\n",
    "    )\n",
    "    init_angle, target_angle = info[8], info[9]\n",
    "    env.set_joint_state(joint, init_angle)\n",
    "    # print(init_angle, target_angle)\n",
    "\n",
    "target_link = available_joints[joint_id]\n",
    "info = p.getJointInfo(\n",
    "    env.render_env.obj_id,\n",
    "    env.render_env.link_name_to_index[target_link],\n",
    "    env.render_env.client_id,\n",
    ")\n",
    "init_angle, target_angle = info[8], info[9]\n",
    "print(init_angle, target_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_obs = env.render(filter_nonobj_pts=True, n_pts=1200)\n",
    "rgb, depth, seg, P_cam, P_world, pc_seg, segmap = pc_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_point(object_id, link_index, world_point):\n",
    "    if link_index == -1:\n",
    "        # Base link (root link)\n",
    "        position, orientation = p.getBasePositionAndOrientation(object_id)\n",
    "    else:\n",
    "        # Specific link\n",
    "        link_state = p.getLinkState(object_id, link_index)\n",
    "        position = link_state[4]  # Link world position\n",
    "        orientation = link_state[5]  # Link world orientation\n",
    "\n",
    "    # Convert orientation to a rotation matrix\n",
    "    rotation_matrix = p.getMatrixFromQuaternion(orientation)\n",
    "    rotation_matrix = np.array(rotation_matrix).reshape(3, 3)\n",
    "\n",
    "    # Transform the world point to local coordinates\n",
    "    local_point = np.dot(\n",
    "        np.linalg.inv(rotation_matrix), (world_point - np.array(position))\n",
    "    )\n",
    "    return local_point\n",
    "\n",
    "\n",
    "def get_world_point(object_id, link_index, local_point):\n",
    "    if link_index == -1:\n",
    "        # Base link (root link)\n",
    "        position, orientation = p.getBasePositionAndOrientation(object_id)\n",
    "    else:\n",
    "        # Specific link\n",
    "        link_state = p.getLinkState(object_id, link_index)\n",
    "        position = link_state[4]  # Link world position\n",
    "        orientation = link_state[5]  # Link world orientation\n",
    "\n",
    "    # Convert orientation to a rotation matrix\n",
    "    rotation_matrix = p.getMatrixFromQuaternion(orientation)\n",
    "    rotation_matrix = np.array(rotation_matrix).reshape(3, 3)\n",
    "\n",
    "    # Transform the local point to world coordinates\n",
    "    world_point = np.dot(rotation_matrix, local_point) + np.array(position)\n",
    "    return world_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
    "animation = FlowNetAnimation()\n",
    "animation.add_trace(\n",
    "    torch.as_tensor(P_world),\n",
    "    # torch.as_tensor([pcd[mask]]),\n",
    "    # torch.as_tensor([flow[mask]]),\n",
    "    torch.as_tensor([P_world]),\n",
    "    torch.as_tensor([np.zeros((1200, 3))]),\n",
    "    \"red\",\n",
    ")\n",
    "fig = animation.animate()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gripper_tip_pos_before = np.array([-0.03, 0.03, 0.93])\n",
    "gripper_object_contact_local = get_local_point(\n",
    "    env.render_env.obj_id,\n",
    "    env.render_env.link_name_to_index[target_link],\n",
    "    gripper_tip_pos_before,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_joint_state(target_link, -1.51019629)\n",
    "pc_obs = env.render(filter_nonobj_pts=True, n_pts=1200)\n",
    "rgb, depth, seg, P_cam, P_world, pc_seg, segmap = pc_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gripper_tip_pos_after = get_world_point(\n",
    "    env.render_env.obj_id,\n",
    "    env.render_env.link_name_to_index[target_link],\n",
    "    gripper_object_contact_local,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gripper_tip_pos_before, gripper_tip_pos_after)\n",
    "delta_gripper = (gripper_tip_pos_before - gripper_tip_pos_after)\n",
    "delta_gripper = delta_gripper / np.linalg.norm(delta_gripper) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from flowbot3d.grasping.agents.flowbot3d import FlowNetAnimation\n",
    "animation = FlowNetAnimation()\n",
    "animation.add_trace(\n",
    "    torch.as_tensor(P_world),\n",
    "    # torch.as_tensor([pcd[mask]]),\n",
    "    # torch.as_tensor([flow[mask]]),\n",
    "    torch.as_tensor([gripper_tip_pos_after[None, :]]),\n",
    "    torch.as_tensor([delta_gripper[None, :]]),\n",
    "    \"red\",\n",
    ")\n",
    "fig = animation.animate()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the simulation process to debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_anything_diffusion.simulations.suction import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for joint in available_joints:  # Close all joints\n",
    "    info = p.getJointInfo(\n",
    "        env.render_env.obj_id,\n",
    "        env.render_env.link_name_to_index[joint],\n",
    "        env.render_env.client_id,\n",
    "    )\n",
    "    init_angle, target_angle = info[8], info[9]\n",
    "    env.set_joint_state(joint, init_angle)\n",
    "    # print(init_angle, target_angle)\n",
    "\n",
    "target_link = available_joints[joint_id]\n",
    "info = p.getJointInfo(\n",
    "    env.render_env.obj_id,\n",
    "    env.render_env.link_name_to_index[target_link],\n",
    "    env.render_env.client_id,\n",
    ")\n",
    "init_angle, target_angle = info[8], info[9]\n",
    "print(init_angle, target_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 30\n",
    "n_pts = 1200\n",
    "save_name = \"8877_link_1\"\n",
    "website = True\n",
    "gui = False\n",
    "gt_model = None\n",
    "\n",
    "torch.set_printoptions(precision=10)  # Set higher precision for PyTorch outputs\n",
    "np.set_printoptions(precision=10)\n",
    "# p.setPhysicsEngineParameter(numSolverIterations=10)\n",
    "# p.setPhysicsEngineParameter(contactBreakingThreshold=0.01, contactSlop=0.001)\n",
    "\n",
    "initial_movement_thres = 0.01\n",
    "max_trial_per_step = 50\n",
    "this_step_trial = 0\n",
    "\n",
    "sim_trajectory = [0.0] + [0] * (n_steps)  # start from 0.05\n",
    "correct_direction_stack = []  # The direction stack\n",
    "\n",
    "if website:\n",
    "    # Flow animation\n",
    "    animation = FlowNetAnimation()\n",
    "\n",
    "# First, reset the environment.\n",
    "env.reset()\n",
    "# Joint information\n",
    "info = p.getJointInfo(\n",
    "    env.render_env.obj_id,\n",
    "    env.render_env.link_name_to_index[target_link],\n",
    "    env.render_env.client_id,\n",
    ")\n",
    "init_angle, target_angle = info[8], info[9]\n",
    "\n",
    "if (\n",
    "    raw_data.category == \"Door\"\n",
    "    and raw_data.semantics.by_name(target_link).type == \"hinge\"\n",
    "):\n",
    "    env.set_joint_state(target_link, init_angle + 0.0 * (target_angle - init_angle))\n",
    "    # env.set_joint_state(target_link, 0.2)\n",
    "\n",
    "if raw_data.semantics.by_name(target_link).type == \"hinge\":\n",
    "    env.set_joint_state(target_link, init_angle + 0.0 * (target_angle - init_angle))\n",
    "    # env.set_joint_state(target_link, 0.05)\n",
    "\n",
    "# Predict the flow on the observation.\n",
    "pc_obs = env.render(filter_nonobj_pts=True, n_pts=n_pts)\n",
    "rgb, depth, seg, P_cam, P_world, pc_seg, segmap = pc_obs\n",
    "\n",
    "if init_angle == target_angle:  # Not movable\n",
    "    p.disconnect(physicsClientId=env.render_env.client_id)\n",
    "    print(\"init == target\")\n",
    "\n",
    "# breakpoint()\n",
    "if gt_model is None:  # GT Flow model\n",
    "    pred_trajectory = model(copy.deepcopy(pc_obs))\n",
    "else:\n",
    "    movable_mask = gt_model.get_movable_mask(pc_obs)\n",
    "    pred_trajectory = model(copy.deepcopy(pc_obs), movable_mask)\n",
    "# pred_trajectory = model(copy.deepcopy(pc_obs))\n",
    "# breakpoint()\n",
    "pred_trajectory = pred_trajectory.reshape(\n",
    "    pred_trajectory.shape[0], -1, pred_trajectory.shape[-1]\n",
    ")\n",
    "traj_len = pred_trajectory.shape[1]  # Trajectory length\n",
    "print(f\"Predicting {traj_len} length trajectories.\")\n",
    "pred_flow = pred_trajectory[:, 0, :]\n",
    "\n",
    "# flow_fig(torch.from_numpy(P_world), pred_flow, sizeref=0.1, use_v2=True).show()\n",
    "# breakpoint()\n",
    "\n",
    "# Filter down just the points on the target link.\n",
    "link_ixs = pc_seg == env.render_env.link_name_to_index[target_link]\n",
    "# assert link_ixs.any()\n",
    "if not link_ixs.any():\n",
    "    p.disconnect(physicsClientId=env.render_env.client_id)\n",
    "    print(\"link_ixs finds no point\")\n",
    "    animation_results = animation.animate() if website else None\n",
    "\n",
    "\n",
    "if website:\n",
    "    if gui:\n",
    "        # Record simulation video\n",
    "        log_id = p.startStateLogging(\n",
    "            p.STATE_LOGGING_VIDEO_MP4,\n",
    "            f\"./logs/simu_eval/video_assets/{save_name}.mp4\",\n",
    "        )\n",
    "    else:\n",
    "        video_file = f\"./logs/simu_eval/video_assets/{save_name}.mp4\"\n",
    "        # # cv2 output videos won't show on website\n",
    "        frame_width = 640\n",
    "        frame_height = 480\n",
    "        # fps = 5\n",
    "        # fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        # videoWriter = cv2.VideoWriter(video_file, fourcc, fps, (frame_width, frame_height))\n",
    "        # videoWriter.write(rgbImgOpenCV)\n",
    "\n",
    "        # Camera param\n",
    "        writer = imageio.get_writer(video_file, fps=5)\n",
    "\n",
    "        # Capture image\n",
    "        width, height, rgbImg, depthImg, segImg = p.getCameraImage(\n",
    "            width=frame_width,\n",
    "            height=frame_height,\n",
    "            viewMatrix=p.computeViewMatrixFromYawPitchRoll(\n",
    "                cameraTargetPosition=[0, 0, 0],\n",
    "                distance=5,\n",
    "                # yaw=270,\n",
    "                yaw=90,\n",
    "                pitch=-30,\n",
    "                roll=0,\n",
    "                upAxisIndex=2,\n",
    "            ),\n",
    "            projectionMatrix=p.computeProjectionMatrixFOV(\n",
    "                fov=60,\n",
    "                aspect=float(frame_width) / frame_height,\n",
    "                nearVal=0.1,\n",
    "                farVal=100.0,\n",
    "            ),\n",
    "        )\n",
    "        image = np.array(rgbImg, dtype=np.uint8)\n",
    "        image = image[:, :, :3]\n",
    "\n",
    "        # Add the frame to the video\n",
    "        writer.append_data(image)\n",
    "\n",
    "# The attachment point is the point with the highest flow.\n",
    "# best_flow_ix = pred_flow[link_ixs].norm(dim=-1).argmax()\n",
    "best_flow_ixs, best_flows, best_points = choose_grasp_points(\n",
    "    pred_flow[link_ixs], P_world[link_ixs], filter_edge=False, k=40\n",
    ")\n",
    "\n",
    "# Teleport to an approach pose, approach, the object and grasp.\n",
    "if website and not gui:\n",
    "    # contact = env.teleport_and_approach(best_point, best_flow, video_writer=writer)\n",
    "    best_flow_ix_id, contact = env.teleport(\n",
    "        best_points, best_flows, video_writer=writer, target_link=target_link\n",
    "    )\n",
    "else:\n",
    "    # contact = env.teleport_and_approach(best_point, best_flow)\n",
    "    best_flow_ix_id, contact = env.teleport(best_points, best_flows, target_link=target_link)\n",
    "\n",
    "best_flow = pred_flow[link_ixs][best_flow_ixs[best_flow_ix_id]]\n",
    "best_point = P_world[link_ixs][best_flow_ixs[best_flow_ix_id]]\n",
    "\n",
    "if not contact:\n",
    "    if website:\n",
    "        segmented_flow = np.zeros_like(pred_flow)\n",
    "        segmented_flow[link_ixs] = pred_flow[link_ixs]\n",
    "        segmented_flow = np.array(\n",
    "            normalize_trajectory(\n",
    "                torch.from_numpy(np.expand_dims(segmented_flow, 1))\n",
    "            ).squeeze()\n",
    "        )\n",
    "        animation.add_trace(\n",
    "            torch.as_tensor(P_world),\n",
    "            torch.as_tensor([P_world]),\n",
    "            torch.as_tensor([segmented_flow]),\n",
    "            \"red\",\n",
    "        )\n",
    "        if gui:\n",
    "            p.stopStateLogging(log_id)\n",
    "        else:\n",
    "            # Write video\n",
    "            writer.close()\n",
    "            # videoWriter.release()\n",
    "\n",
    "    print(\"No contact!\")\n",
    "    p.disconnect(physicsClientId=env.render_env.client_id)\n",
    "    animation_results = None if not website else animation.animate()\n",
    "\n",
    "env.attach()\n",
    "use_history = False\n",
    "# gripper_tip_pos_before, _ = p.getBasePositionAndOrientation(env.gripper.base_id)\n",
    "# points = p.getContactPoints(bodyA=env.gripper.body_id, bodyB=env.render_env.obj_id, linkIndexA=0)\n",
    "# assert len(points)!=0, \"Contact is None!!!!\"\n",
    "# gripper_tip_pos_before, _ = points[0][5], points[0][6]\n",
    "gripper_tip_pos_before = best_point\n",
    "gripper_object_contact_local = get_local_point(\n",
    "    env.render_env.obj_id,\n",
    "    env.render_env.link_name_to_index[target_link],\n",
    "    gripper_tip_pos_before,\n",
    ")\n",
    "# print(gripper_tip_pos_before, gripper_object_contact_local, get_world_point(env.render_env.obj_id, env.render_env.link_name_to_index[target_link], gripper_object_contact_local))\n",
    "# env.pull(best_flow)\n",
    "reset = env.pull_with_constraint(best_flow, target_link=target_link)\n",
    "if not reset:\n",
    "    env.attach()\n",
    "    gripper_tip_pos_after = get_world_point(\n",
    "        env.render_env.obj_id,\n",
    "        env.render_env.link_name_to_index[target_link],\n",
    "        gripper_object_contact_local,\n",
    "    )\n",
    "\n",
    "    delta_gripper = np.array(gripper_tip_pos_after) - np.array(\n",
    "        gripper_tip_pos_before\n",
    "    )\n",
    "    last_step_grasp_point = best_point\n",
    "    \n",
    "    if np.linalg.norm(delta_gripper) > 1e-6:  # Because\n",
    "        correct_direction_stack.append(delta_gripper)\n",
    "    # Judge whether the movement is\n",
    "    if np.linalg.norm(delta_gripper) > initial_movement_thres:\n",
    "        use_history = True\n",
    "        prev_flow_pred = pred_flow.clone()  # History flow\n",
    "        prev_point_cloud = copy.deepcopy(P_world)  # History point cloud\n",
    "\n",
    "        # correct_direction_stack.append(delta_gripper)\n",
    "        # print(\"Pushing to correct direction stack:::::\", np.linalg.norm(delta_gripper), delta_gripper / np.linalg.norm(delta_gripper), best_flow / np.linalg.norm(best_flow), np.dot(delta_gripper / np.linalg.norm(delta_gripper), best_flow / np.linalg.norm(best_flow)))\n",
    "else:  # Need a reset because hit the lower boundary - definitely not a good step\n",
    "    use_history = False\n",
    "    last_step_grasp_point = None  # No contact anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"current stack top: \", np.array(correct_direction_stack[-1]) / np.linalg.norm(np.array(correct_direction_stack[-1])))\n",
    "print(\"best flow: \", best_flow / np.linalg.norm(best_flow.numpy()))\n",
    "print(\"cosine:\", np.dot(np.array(correct_direction_stack[-1]) / np.linalg.norm(np.array(correct_direction_stack[-1])), best_flow.numpy() / np.linalg.norm(best_flow.numpy())))\n",
    "success, sim_trajectory[0] = env.detect_success(target_link)\n",
    "print(\"Current angle:\", sim_trajectory[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# breakpoint()\n",
    "global_step = 1\n",
    "\n",
    "# for i in range(n_steps):\n",
    "while not success and global_step < n_steps:\n",
    "    pc_obs = env.render(\n",
    "        filter_nonobj_pts=True, n_pts=n_pts\n",
    "    )  # Render a new point cloud!  #\n",
    "    # Predict the flow on the observation.\n",
    "    if gt_model is None:  # GT Flow model\n",
    "        if use_history:\n",
    "            print(\"Using history!\")\n",
    "            # Use history model\n",
    "            pred_trajectory = model_with_history(\n",
    "                copy.deepcopy(pc_obs),\n",
    "                copy.deepcopy(prev_point_cloud),\n",
    "                copy.deepcopy(prev_flow_pred.numpy()),\n",
    "            )\n",
    "        else:\n",
    "            pred_trajectory = model(copy.deepcopy(pc_obs))\n",
    "    else:\n",
    "        movable_mask = gt_model.get_movable_mask(pc_obs)\n",
    "        # breakpoint()\n",
    "        pred_trajectory = model(pc_obs, movable_mask)\n",
    "        # pred_trajectory = model(pc_obs)\n",
    "    pred_trajectory = pred_trajectory.reshape(\n",
    "        pred_trajectory.shape[0], -1, pred_trajectory.shape[-1]\n",
    "    )\n",
    "\n",
    "    pred_flow = pred_trajectory[:, 0, :]\n",
    "    rgb, depth, seg, P_cam, P_world, pc_seg, segmap = pc_obs\n",
    "\n",
    "    # Filter down just the points on the target link.\n",
    "    # breakpoint()\n",
    "    link_ixs = pc_seg == env.render_env.link_name_to_index[target_link]\n",
    "    # assert link_ixs.any()\n",
    "    if not link_ixs.any():\n",
    "        if website:\n",
    "            if gui:\n",
    "                p.stopStateLogging(log_id)\n",
    "            else:\n",
    "                writer.close()\n",
    "                # videoWriter.release()\n",
    "        p.disconnect(physicsClientId=env.render_env.client_id)\n",
    "        print(\"link_ixs finds no point\")\n",
    "        animation_results = animation.animate() if website else None\n",
    "        return (\n",
    "            animation_results,\n",
    "            TrialResult(\n",
    "                assertion=False,\n",
    "                success=False,\n",
    "                contact=False,\n",
    "                init_angle=0,\n",
    "                final_angle=0,\n",
    "                now_angle=0,\n",
    "                metric=0,\n",
    "            ),\n",
    "            sim_trajectory,\n",
    "        )\n",
    "\n",
    "    # Get the best direction.\n",
    "    # best_flow_ix = pred_flow[link_ixs].norm(dim=-1).argmax()\n",
    "    # ------------DEBUG-------------\n",
    "    gt_model_debug = GTTrajectoryModel(raw_data, env, 1)\n",
    "    gt_flow = gt_model_debug.get_gt_force_vector(pc_obs, link_ixs)\n",
    "    if len(correct_direction_stack) != 0:\n",
    "        print(\"GT flow's cosine with the last consistent vector!!!!!\", np.dot(gt_flow.numpy(), correct_direction_stack[-1] / (np.linalg.norm(correct_direction_stack[-1]) + 1e-6)))\n",
    "    # ------------DEBUG-------------\n",
    "\n",
    "\n",
    "    best_flow_ixs, best_flows, best_points = choose_grasp_points(\n",
    "        pred_flow[link_ixs],\n",
    "        P_world[link_ixs],\n",
    "        filter_edge=False,\n",
    "        k=40,\n",
    "        last_correct_direction=None\n",
    "        if len(correct_direction_stack) == 0\n",
    "        else correct_direction_stack[-1],\n",
    "    )\n",
    "    have_to_execute_incorrect = False\n",
    "\n",
    "    if (\n",
    "        len(best_flows) == 0\n",
    "    ):  # All top 20 points are filtered out! - Not a good prediction - move on!\n",
    "        this_step_trial += 1\n",
    "        if (\n",
    "            this_step_trial > max_trial_per_step\n",
    "        ):  # To make the process go on, must make an action!\n",
    "            have_to_execute_incorrect = True\n",
    "            print(\"has to execute incorrect!!!\")\n",
    "            best_flow_ixs, best_flows, best_points = choose_grasp_points(\n",
    "                pred_flow[link_ixs],\n",
    "                P_world[link_ixs],\n",
    "                filter_edge=False,\n",
    "                k=20,\n",
    "                last_correct_direction=None,\n",
    "            )\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # (1) Strategy 1 - Don't change grasp point\n",
    "    # (2) Strategy 2 - Change grasp point when leverage difference is large\n",
    "    lev_diff_thres = 0.2\n",
    "    no_movement_thres = -1\n",
    "\n",
    "    # # Don't switch grasp point\n",
    "    # lev_diff_thres = 100\n",
    "    # no_movement_thres = -1\n",
    "    # good_movement_thres = 1000\n",
    "\n",
    "    if last_step_grasp_point is not None:  # Still grasping!\n",
    "        gripper_tip_pos, _ = p.getBasePositionAndOrientation(env.gripper.body_id)\n",
    "        pcd_dist = torch.tensor(P_world[link_ixs] - np.array(gripper_tip_pos)).norm(\n",
    "            dim=-1\n",
    "        )\n",
    "        grasp_point_id = pcd_dist.argmin()\n",
    "        lev_diff = best_flows.norm(dim=-1) - pred_flow[link_ixs][\n",
    "            grasp_point_id\n",
    "        ].norm(dim=-1)\n",
    "\n",
    "    if (  # need to switch grasp point\n",
    "        last_step_grasp_point is None or lev_diff[0] > lev_diff_thres\n",
    "    ):\n",
    "        env.reset_gripper()\n",
    "        p.stepSimulation(\n",
    "            env.render_env.client_id\n",
    "        )  # Make sure the constraint is lifted\n",
    "\n",
    "        if website and not gui:\n",
    "            # contact = env.teleport_and_approach(best_point, best_flow, video_writer=writer)\n",
    "            best_flow_ix_id, contact = env.teleport(\n",
    "                best_points, best_flows, video_writer=writer\n",
    "            )\n",
    "        else:\n",
    "            # contact = env.teleport_and_approach(best_point, best_flow)\n",
    "            best_flow_ix_id, contact = env.teleport(best_points, best_flows)\n",
    "        best_flow = pred_flow[link_ixs][best_flow_ixs[best_flow_ix_id]]\n",
    "        best_point = P_world[link_ixs][best_flow_ixs[best_flow_ix_id]]\n",
    "        last_step_grasp_point = best_point  # Grasp a new point\n",
    "        # print(\"new!\", last_step_grasp_point)\n",
    "\n",
    "        if not contact:\n",
    "            if website:\n",
    "                segmented_flow = np.zeros_like(pred_flow)\n",
    "                segmented_flow[link_ixs] = pred_flow[link_ixs]\n",
    "                segmented_flow = np.array(\n",
    "                    normalize_trajectory(\n",
    "                        torch.from_numpy(np.expand_dims(segmented_flow, 1))\n",
    "                    ).squeeze()\n",
    "                )\n",
    "                animation.add_trace(\n",
    "                    torch.as_tensor(P_world),\n",
    "                    torch.as_tensor([P_world]),\n",
    "                    torch.as_tensor([segmented_flow]),\n",
    "                    \"red\",\n",
    "                )\n",
    "                if gui:\n",
    "                    p.stopStateLogging(log_id)\n",
    "                else:\n",
    "                    # Write video\n",
    "                    writer.close()\n",
    "                    # videoWriter.release()\n",
    "\n",
    "            print(\"No contact!\")\n",
    "            p.disconnect(physicsClientId=env.render_env.client_id)\n",
    "            animation_results = None if not website else animation.animate()\n",
    "            return (\n",
    "                animation_results,\n",
    "                TrialResult(\n",
    "                    success=False,\n",
    "                    assertion=True,\n",
    "                    contact=False,\n",
    "                    init_angle=0,\n",
    "                    final_angle=0,\n",
    "                    now_angle=0,\n",
    "                    metric=0,\n",
    "                ),\n",
    "                sim_trajectory,\n",
    "            )\n",
    "\n",
    "        env.attach()\n",
    "    else:  # Stick to the old grasp point\n",
    "        best_flow = pred_flow[link_ixs][best_flow_ixs[0]]\n",
    "        best_point = P_world[link_ixs][grasp_point_id]\n",
    "        last_step_grasp_point = (\n",
    "            best_point  # The original point - don't need to change\n",
    "        )\n",
    "        # print(\"same:\", last_step_grasp_point)\n",
    "\n",
    "    # Execute the step:\n",
    "    print(\"GT flow's cosine with the predicted vector!!!!!\", gt_flow, best_flow / (np.linalg.norm(best_flow) + 1e-6), np.dot(gt_flow.numpy(), best_flow / (np.linalg.norm(best_flow) + 1e-6)))\n",
    "    env.attach()\n",
    "    # gripper_tip_pos_before, _ = p.getBasePositionAndOrientation(env.gripper.base_id)\n",
    "    gripper_tip_pos_before = last_step_grasp_point\n",
    "    gripper_object_contact_local = get_local_point(\n",
    "        env.render_env.obj_id,\n",
    "        env.render_env.link_name_to_index[target_link],\n",
    "        gripper_tip_pos_before,\n",
    "    )\n",
    "    reset = env.pull_with_constraint(best_flow, target_link=target_link)\n",
    "    # -------DEBUG-------\n",
    "    # print(gt_flow)\n",
    "    # reset = env.pull_with_constraint(gt_flow, target_link=target_link)\n",
    "    # -------DEBUG-------\n",
    "    if not reset:\n",
    "        env.attach()\n",
    "        gripper_tip_pos_after = get_world_point(\n",
    "            env.render_env.obj_id,\n",
    "            env.render_env.link_name_to_index[target_link],\n",
    "            gripper_object_contact_local,\n",
    "        )\n",
    "\n",
    "        # Now with filter: we guarantee that every step is correct!!\n",
    "        delta_gripper = np.array(gripper_tip_pos_after) - np.array(\n",
    "            gripper_tip_pos_before\n",
    "        )\n",
    "        # print(delta_gripper, np.linalg.norm(delta_gripper))\n",
    "        if (\n",
    "            len(correct_direction_stack) == 0\n",
    "            and np.linalg.norm(delta_gripper) < initial_movement_thres\n",
    "        ):  # Still waiting for the initial movement!!!\n",
    "            use_history = False\n",
    "        else:\n",
    "            last_step_grasp_point = best_point\n",
    "            if len(correct_direction_stack) == 0 or (\n",
    "                np.dot(delta_gripper, correct_direction_stack[-1]) > 0\n",
    "                and np.linalg.norm(delta_gripper) > initial_movement_thres\n",
    "            ):  # The actual move direction is consistent with the previous movements\n",
    "                use_history = True\n",
    "                if have_to_execute_incorrect and (\n",
    "                    np.dot(delta_gripper, correct_direction_stack[-1]) > 0\n",
    "                ):\n",
    "                    print(\"Doesn't satisfy cosine condition, but is correct!\")\n",
    "                prev_flow_pred = pred_flow.clone()  # History flow\n",
    "                prev_point_cloud = copy.deepcopy(P_world)  # History point cloud\n",
    "\n",
    "                correct_direction_stack.append(\n",
    "                    delta_gripper / (np.linalg.norm(delta_gripper) + 1e-6)\n",
    "                )\n",
    "    else:  # Reset\n",
    "        use_history = False\n",
    "        last_step_grasp_point = None\n",
    "\n",
    "    global_step += 1\n",
    "    this_step_trial = 0\n",
    "\n",
    "    if website:\n",
    "        # Add pcd to flow animation\n",
    "        segmented_flow = np.zeros_like(pred_flow)\n",
    "        segmented_flow[link_ixs] = pred_flow[link_ixs]\n",
    "        segmented_flow = np.array(\n",
    "            normalize_trajectory(\n",
    "                torch.from_numpy(np.expand_dims(segmented_flow, 1))\n",
    "            ).squeeze()\n",
    "        )\n",
    "        animation.add_trace(\n",
    "            torch.as_tensor(P_world),\n",
    "            torch.as_tensor([P_world]),\n",
    "            torch.as_tensor([segmented_flow]),\n",
    "            \"red\",\n",
    "        )\n",
    "\n",
    "        # Capture frame\n",
    "        width, height, rgbImg, depthImg, segImg = p.getCameraImage(\n",
    "            width=frame_width,\n",
    "            height=frame_height,\n",
    "            viewMatrix=p.computeViewMatrixFromYawPitchRoll(\n",
    "                cameraTargetPosition=[0, 0, 0],\n",
    "                distance=5,\n",
    "                # yaw=270,\n",
    "                yaw=90,\n",
    "                pitch=-30,\n",
    "                roll=0,\n",
    "                upAxisIndex=2,\n",
    "            ),\n",
    "            projectionMatrix=p.computeProjectionMatrixFOV(\n",
    "                fov=60,\n",
    "                aspect=float(frame_width) / frame_height,\n",
    "                nearVal=0.1,\n",
    "                farVal=100.0,\n",
    "            ),\n",
    "        )\n",
    "        # rgbImgOpenCV = cv2.cvtColor(np.array(rgbImg), cv2.COLOR_RGB2BGR)\n",
    "        # videoWriter.write(rgbImgOpenCV)\n",
    "        image = np.array(rgbImg, dtype=np.uint8)\n",
    "        image = image[:, :, :3]\n",
    "\n",
    "        # Add the frame to the video\n",
    "        writer.append_data(image)\n",
    "\n",
    "    success, sim_trajectory[global_step] = env.detect_success(target_link)\n",
    "\n",
    "    if success:\n",
    "        break\n",
    "\n",
    "    # pc_obs = env.render(filter_nonobj_pts=True, n_pts=1200)   # Render a new point cloud!\n",
    "    # if len(correct_direction_stack) == 2:\n",
    "    #     breakpoint()\n",
    "\n",
    "for left_step in range(global_step, 31):\n",
    "    sim_trajectory[left_step] = sim_trajectory[global_step]\n",
    "# calculate the metrics\n",
    "# if success:\n",
    "#     metric = 1\n",
    "# else:\n",
    "#     curr_pos = env.get_joint_value(target_link)\n",
    "#     metric = (curr_pos - init_angle) / (target_angle - init_angle)\n",
    "#     metric = min(max(metric, 0), 1)\n",
    "curr_pos = env.get_joint_value(target_link)\n",
    "metric = (curr_pos - init_angle) / (target_angle - init_angle)\n",
    "metric = min(max(metric, 0), 1)\n",
    "\n",
    "if website:\n",
    "    if gui:\n",
    "        p.stopStateLogging(log_id)\n",
    "    else:\n",
    "        writer.close()\n",
    "        # videoWriter.release()\n",
    "\n",
    "p.disconnect(physicsClientId=env.render_env.client_id)\n",
    "animation_results = None if not website else animation.animate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openany",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
